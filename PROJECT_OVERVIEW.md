# üìã T·ªïng H·ª£p Project: NLP Equipment Fault Analysis System

## 1. M√¥ t·∫£ t·ªïng quan

ƒê√¢y l√† h·ªá th·ªëng **ph√¢n t√≠ch c·∫£nh b√°o l·ªói thi·∫øt b·ªã c√¥ng nghi·ªáp** s·ª≠ d·ª•ng **NLP (Natural Language Processing)** v·ªõi ki·∫øn tr√∫c **Multi-Engine**. H·ªá th·ªëng nh·∫≠n ƒë·∫ßu v√†o l√† **m√¥ t·∫£ b·∫±ng ng√¥n ng·ªØ ti·∫øng Vi·ªát** v·ªÅ t√¨nh tr·∫°ng thi·∫øt b·ªã, sau ƒë√≥ t·ª± ƒë·ªông ph√¢n lo·∫°i l·ªói v√†o **10 nh√≥m l·ªói**, ƒë√°nh gi√° m·ª©c ƒë·ªô nghi√™m tr·ªçng, v√† ƒë∆∞a ra khuy·∫øn ngh·ªã x·ª≠ l√Ω.

H·ªá th·ªëng h·ªó tr·ª£ **2 engine** song song:
- **PhoBERT Engine**: S·ª≠ d·ª•ng model `vinai/phobert-base` fine-tuned th√†nh classifier ‚Äî khai th√°c **semantic understanding** cho ti·∫øng Vi·ªát
- **TF-IDF Engine**: S·ª≠ d·ª•ng TF-IDF Vectorizer + Logistic Regression ‚Äî ph∆∞∆°ng ph√°p **statistical baseline** truy·ªÅn th·ªëng

---

## 2. Ki·∫øn tr√∫c h·ªá th·ªëng

```mermaid
graph TB
    subgraph Frontend
        UI["ui.html<br/>Giao di·ªán web"]
    end

    subgraph Backend["Backend (FastAPI)"]
        APP["app.py<br/>API Routes"]
        FACTORY["engine_factory.py<br/>Engine Factory"]
        BASE["base_engine.py<br/>Abstract Interface"]
        PHOBERT["phobert_engine.py<br/>PhoBERT Engine"]
        TFIDF["tfidf_engine.py<br/>TF-IDF Engine"]
        DB["database.py<br/>SQLite Database"]
        SCH["schemas.py<br/>Pydantic Models"]
    end

    subgraph Training["Training Pipeline"]
        PREP["data_preparation.py<br/>Data Loading & Augmentation"]
        TRAIN_P["train_phobert.py<br/>Fine-tune PhoBERT"]
        TRAIN_T["train_tfidf.py<br/>Train TF-IDF"]
        CFG["config.py<br/>Hyperparameters"]
    end

    subgraph Evaluation["Evaluation Framework"]
        EVAL["run_evaluation.py<br/>Engine Comparator"]
        METRICS["metrics.py<br/>Classification Metrics"]
        LATENCY["latency.py<br/>Latency Benchmark"]
    end

    subgraph Resources
        MODEL_BASE["phobert-base/<br/>Pre-trained Model"]
        MODEL_FT["phobert-finetuned/<br/>Fine-tuned Classifier"]
        MODEL_TFIDF["tfidf/<br/>TF-IDF Artifacts"]
        DBFILE["history.db<br/>SQLite DB"]
    end

    UI -->|"POST /analyze"| APP
    UI -->|"GET /history"| APP
    APP --> FACTORY
    FACTORY --> PHOBERT
    FACTORY --> TFIDF
    PHOBERT --> BASE
    TFIDF --> BASE
    PHOBERT --> MODEL_BASE
    PHOBERT --> MODEL_FT
    TFIDF --> MODEL_TFIDF
    APP --> DB
    DB --> DBFILE
    PREP --> TRAIN_P
    PREP --> TRAIN_T
    CFG --> TRAIN_P
    CFG --> TRAIN_T
    EVAL --> PHOBERT
    EVAL --> TFIDF
    EVAL --> METRICS
    EVAL --> LATENCY
```

---

## 3. C·∫•u tr√∫c th∆∞ m·ª•c

```
nlp/
‚îú‚îÄ‚îÄ main.py                              # Entry point ‚Äî Uvicorn server (port 10805)
‚îú‚îÄ‚îÄ test_e2e.py                          # E2E tests
‚îú‚îÄ‚îÄ requirements.txt                     # Python dependencies
‚îú‚îÄ‚îÄ PROJECT_OVERVIEW.md                  # T√†i li·ªáu t·ªïng quan (file n√†y)
‚îú‚îÄ‚îÄ EVALUATION_REPORT.md                 # B√°o c√°o ƒë√°nh gi√°
‚îÇ
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app.py                           # FastAPI app + API routes
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_engine.py               # ‚≠ê Abstract BaseNLPEngine + AnalysisResult
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ engine_factory.py            # Factory pattern ‚Äî t·∫°o engine theo t√™n
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ phobert_engine.py            # ‚≠ê PhoBERT Engine (fine-tuned classifier)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tfidf_engine.py              # TF-IDF Engine (Logistic Regression)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nlp_engine.py               # Legacy engine (deprecated)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py                    # Hyperparameters cho c·∫£ 2 engine
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_preparation.py          # Load data + augmentation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_phobert.py             # ‚≠ê Pipeline fine-tune PhoBERT
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_tfidf.py              # Pipeline train TF-IDF
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluation_dataset.json      # 149 m·∫´u test (10 classes √ó ~15 m·∫´u)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run_evaluation.py            # ‚≠ê So s√°nh engines (accuracy, F1, latency)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py                   # Confusion matrix, classification report
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ latency.py                   # Benchmark latency (mean, P95, min, max)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ results/                     # Output: reports, confusion matrices
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database.py                  # SQLite CRUD operations
‚îÇ   ‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py                   # Pydantic request/response models
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ resources/
‚îÇ       ‚îú‚îÄ‚îÄ phobert-base/                # PhoBERT pre-trained weights (540MB)
‚îÇ       ‚îú‚îÄ‚îÄ phobert-finetuned/           # ‚≠ê Fine-tuned classifier artifacts
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ model.pt                 # Full model state_dict (540MB)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ classifier_head.pt       # Classification head weights
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ label_encoder.pkl        # Label encoder (10 classes)
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ metadata.json            # Training metadata & metrics
‚îÇ       ‚îú‚îÄ‚îÄ tfidf/                       # TF-IDF vectorizer + classifier
‚îÇ       ‚îî‚îÄ‚îÄ database/history.db          # SQLite database
‚îÇ
‚îî‚îÄ‚îÄ frontend/
    ‚îî‚îÄ‚îÄ templates/
        ‚îî‚îÄ‚îÄ ui.html                      # Single-page web UI
```

---

## 4. Multi-Engine Architecture

### 4.1 Design Pattern

H·ªá th·ªëng √°p d·ª•ng **Strategy Pattern** v·ªõi abstract base class:

```python
class BaseNLPEngine(ABC):
    """Interface chu·∫©n cho m·ªçi NLP engine."""
    @property
    @abstractmethod
    def name(self) -> str: ...

    @abstractmethod
    def analyze(self, equipment: str, description: str) -> AnalysisResult: ...
```

M·ªçi engine ƒë·ªÅu tr·∫£ v·ªÅ **`AnalysisResult`** c√≥ c√πng c·∫•u tr√∫c, cho ph√©p frontend hi·ªÉn th·ªã th·ªëng nh·∫•t v√† evaluation so s√°nh c√¥ng b·∫±ng.

### 4.2 So s√°nh 2 Engines

| ƒê·∫∑c ƒëi·ªÉm | PhoBERT Engine | TF-IDF Engine |
|---|---|---|
| **Ph∆∞∆°ng ph√°p** | Deep Learning (Transformer) | Statistical ML |
| **Model** | PhoBERT-base + Fine-tuned Linear Head | TF-IDF + Logistic Regression |
| **S·ªë tham s·ªë** | ~135M (PhoBERT) + 7,690 (head) | ~50K (vectorizer + classifier) |
| **Input** | Raw text ‚Üí Tokenize ‚Üí Embedding 768-dim | Raw text ‚Üí TF-IDF features |
| **Training** | Fine-tune to√†n b·ªô PhoBERT + classification head | Train TF-IDF vectorizer + classifier |
| **∆Øu ƒëi·ªÉm** | Hi·ªÉu ng·ªØ nghƒ©a, x·ª≠ l√Ω c√¢u ph·ª©c t·∫°p | Nhanh, nh·∫π, d·ªÖ interpret |
| **Nh∆∞·ª£c ƒëi·ªÉm** | Ch·∫≠m h∆°n (~30ms/sample), c·∫ßn GPU | Kh√¥ng hi·ªÉu semantic, y·∫øu v·ªõi paraphrase |

---

## 5. PhoBERT Engine ‚Äî Chi ti·∫øt k·ªπ thu·∫≠t

### 5.1 NLP Pipeline (6 b∆∞·ªõc)

```mermaid
graph LR
    A["üìù VƒÉn b·∫£n<br/>ti·∫øng Vi·ªát"] --> B["1. Ti·ªÅn x·ª≠ l√Ω"]
    B --> C["2. PhoBERT<br/>Tokenization"]
    C --> D["3. Tr√≠ch xu·∫•t<br/>t·ª´ kh√≥a"]
    D --> E["4. Ph√¢n lo·∫°i l·ªói<br/>Fine-tuned Classifier"]
    E --> F["5. ƒê√°nh gi√°<br/>m·ª©c ƒë·ªô"]
    F --> G["6. Sinh<br/>khuy·∫øn ngh·ªã"]
```

### 5.2 B∆∞·ªõc 1 ‚Äî Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n
- Normalize Unicode (NFC)
- Chuy·ªÉn lowercase
- Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, gi·ªØ l·∫°i ti·∫øng Vi·ªát c√≥ d·∫•u
- Chu·∫©n h√≥a kho·∫£ng tr·∫Øng

### 5.3 B∆∞·ªõc 2 ‚Äî PhoBERT Tokenization & Encoding
- Tokenize text b·∫±ng PhoBERT tokenizer (`vinai/phobert-base`)
- Encode th√†nh **embedding vector 768 chi·ªÅu** s·ª≠ d·ª•ng `[CLS]` token t·ª´ last hidden state
- `max_length = 256` tokens, padding + truncation

### 5.4 B∆∞·ªõc 3 ‚Äî Tr√≠ch xu·∫•t t·ª´ kh√≥a (Keyword Extraction)
- So kh·ªõp vƒÉn b·∫£n v·ªõi **SYMPTOM_KEYWORDS** database (8 nh√≥m: Nhi·ªát ƒë·ªô, Rung ƒë·ªông, √Çm thanh, M√πi, ƒêi·ªán, R√≤ r·ªâ, C∆° kh√≠, Hi·ªáu su·∫•t)
- X·ª≠ l√Ω **negation** (ph·ªß ƒë·ªãnh): nh·∫≠n bi·∫øt "kh√¥ng n√≥ng", "kh√¥ng rung" ‚Üí lo·∫°i b·ªè keyword b·ªã ph·ªß ƒë·ªãnh
- Tr√°nh overlap gi·ªØa c√°c keyword ƒë√£ detect

### 5.5 B∆∞·ªõc 4 ‚Äî Ph√¢n lo·∫°i l·ªói (Dual-Mode Classification)

ƒê√¢y l√† **b∆∞·ªõc c·ªët l√µi** ‚Äî h·ªá th·ªëng h·ªó tr·ª£ **2 ch·∫ø ƒë·ªô ph√¢n lo·∫°i**:

#### Ch·∫ø ƒë·ªô 1: Fine-tuned Classifier (m·∫∑c ƒë·ªãnh khi c√≥ model ƒë√£ train)

```
Input text ‚Üí PhoBERT Encoder ‚Üí [CLS] embedding (768-dim)
    ‚Üí Dropout(0.3) ‚Üí Linear(768, 10) ‚Üí Softmax ‚Üí Top class
```

- S·ª≠ d·ª•ng **softmax probabilities** tr·ª±c ti·∫øp ‚Äî kh√¥ng c·∫ßn heuristics b·ªï sung
- 10 output classes t∆∞∆°ng ·ª©ng 10 lo·∫°i l·ªói
- Confidence score = softmax probability c·ªßa class cao nh·∫•t

#### Ch·∫ø ƒë·ªô 2: Zero-shot Cosine Similarity (fallback)

1. **Pre-compute**: T√≠nh tr∆∞·ªõc embeddings cho 10 lo·∫°i l·ªói tham chi·∫øu (m·ªói lo·∫°i 5-15 c√¢u m·∫´u)
2. **Cosine Similarity**: So s√°nh embedding input v·ªõi embedding trung b√¨nh
3. **Keyword-aware Re-ranking**: Boost score c√°c lo·∫°i l·ªói li√™n quan ƒë·∫øn keyword (+0.1/category)
4. **Decision Logic**: N·∫øu top result l√† "B√¨nh th∆∞·ªùng" nh∆∞ng c√≥ keyword ‚Üí ch·ªçn fault type ti·∫øp theo

| STT | Lo·∫°i l·ªói | Severity Base |
|:---:|---|:---:|
| 1 | Ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh (B√¨nh th∆∞·ªùng) | 0.0 |
| 2 | R√≤ r·ªâ h·ªá th·ªëng | 0.5 |
| 3 | √Çm thanh b·∫•t th∆∞·ªùng | 0.5 |
| 4 | Gi·∫£m hi·ªáu su·∫•t | 0.55 |
| 5 | H∆∞ h·ªèng c∆° kh√≠ | 0.6 |
| 6 | Qu√° nhi·ªát | 0.7 |
| 7 | H·ªèng b·∫°c ƒë·∫°n / v√≤ng bi | 0.75 |
| 8 | Qu√° t·∫£i c∆° kh√≠ | 0.8 |
| 9 | S·ª± c·ªë ƒëi·ªán | 0.85 |
| 10 | Ch√°y cu·ªôn d√¢y / ch√°y motor | 0.9 |

### 5.6 B∆∞·ªõc 5 ‚Äî ƒê√°nh gi√° m·ª©c ƒë·ªô nghi√™m tr·ªçng

C√¥ng th·ª©c: `severity_score = severity_base √ó confidence + keyword_bonus`
- `keyword_bonus = min(s·ªë_keyword √ó 0.05, 0.2)`

| Score | M·ª©c ƒë·ªô |
|---|---|
| ‚â• 0.65 | üî¥ **NGHI√äM TR·ªåNG** |
| ‚â• 0.40 | üü° **C·∫¢NH B√ÅO** |
| < 0.40 | üü¢ **B√åNH TH∆Ø·ªúNG** |

### 5.7 B∆∞·ªõc 6 ‚Äî Sinh khuy·∫øn ngh·ªã
- Tra c·ª©u `RECOMMENDATIONS_DB` theo lo·∫°i l·ªói ƒë√£ ph√¢n lo·∫°i
- M·ªói lo·∫°i l·ªói c√≥ 4-6 khuy·∫øn ngh·ªã x·ª≠ l√Ω c·ª• th·ªÉ

---

## 6. Fine-tuning PhoBERT ‚Äî Ph∆∞∆°ng ph√°p v√† K·∫øt qu·∫£

### 6.1 T·∫°i sao c·∫ßn Fine-tuning?

PhoBERT ·ªü ch·∫ø ƒë·ªô **zero-shot** (cosine similarity) ch·ªâ ƒë·∫°t **73.83% accuracy** tr√™n b·ªô evaluation. Nguy√™n nh√¢n:

- **Kh√¥ng ph√¢n bi·ªát ng·ªØ nghƒ©a ph·ªß ƒë·ªãnh**: "Motor kh√¥ng n√≥ng" v√† "Motor n√≥ng" c√≥ embedding g·∫ßn nhau v√¨ PhoBERT ch∆∞a ƒë∆∞·ª£c train ƒë·ªÉ ph√¢n lo·∫°i
- **Embedding space kh√¥ng optimize cho fault classification**: Pre-trained embedding t·ªëi ∆∞u cho language modeling, kh√¥ng ph·∫£i fault classification
- **Heuristics (keyword boosting, decision logic) c√≥ gi·ªõi h·∫°n**: Rule-based kh√¥ng th·ªÉ thay th·∫ø supervised learning

### 6.2 Ki·∫øn tr√∫c Fine-tuned Model

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PhoBERTClassifier (nn.Module)                    ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ PhoBERT Encoder (vinai/phobert-base)       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   12 Transformer layers                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Hidden size: 768                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Attention heads: 12                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Parameters: ~135M                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   ‚ùÑÔ∏è Embedding layer: FROZEN              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                 ‚îÇ [CLS] token (768-dim)          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Dropout(p=0.3)                             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                 ‚îÇ                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Linear(768, 10) ‚Äî Classification Head      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Parameters: 768 √ó 10 + 10 = 7,690       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                 ‚îÇ logits (10-dim)                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Softmax ‚Üí Probabilities (10 classes)       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 6.3 Hyperparameters

| Parameter | Gi√° tr·ªã | Gi·∫£i th√≠ch |
|---|:---:|---|
| Epochs | 20 (early stopping) | Max epochs, d·ª´ng s·ªõm n·∫øu val F1 kh√¥ng c·∫£i thi·ªán |
| Learning Rate | 2e-5 | Standard cho fine-tuning Transformer |
| Batch Size | 8 | Ph√π h·ª£p GPU memory |
| Warmup Ratio | 0.1 | 10% steps ƒë·∫ßu tƒÉng d·∫ßn LR |
| Weight Decay | 0.01 | L2 regularization (AdamW) |
| Dropout | 0.3 | Regularization cho classification head |
| Max Length | 256 | Max tokens per input |
| Freeze Embeddings | True | Gi·ªØ nguy√™n embedding layer, ti·∫øt ki·ªám memory |
| Early Stopping Patience | 5 | D·ª´ng n·∫øu 5 epochs li√™n ti·∫øp kh√¥ng c·∫£i thi·ªán |
| Optimizer | AdamW | Adam with decoupled weight decay |
| LR Scheduler | Linear warmup + Cosine decay | Warmup tr√°nh gradient explosion, cosine decay gi·∫£m LR m∆∞·ª£t |

### 6.4 D·ªØ li·ªáu Training

| Ngu·ªìn | M√¥ t·∫£ | S·ªë l∆∞·ª£ng |
|---|---|:---:|
| FAULT_REFERENCES | C√¢u m·∫´u tham chi·∫øu cho m·ªói lo·∫°i l·ªói | ~150 |
| test_dataset.json | B·ªô test dataset m·ªü r·ªông | ~350 |
| Data Augmentation | Thay th·∫ø synonym (h√†ng trƒÉm t·ª´ ƒë·ªìng nghƒ©a) | ~700 |
| **T·ªïng** | | **1,197** |

- Train/Val split: **80%/20%** (957 train / 240 val)
- Stratified split: ƒë·∫£m b·∫£o t·ª∑ l·ªá class c√¢n b·∫±ng
- Random seed: 42 (reproducibility)

### 6.5 Training Process

```
Epoch 1: Train Loss=1.8234, Val Acc=0.8500, Val F1=0.8456
Epoch 2: Train Loss=0.5123, Val Acc=0.9458, Val F1=0.9423
Epoch 3: Train Loss=0.1876, Val Acc=0.9833, Val F1=0.9831
Epoch 4: Train Loss=0.0892, Val Acc=1.0000, Val F1=1.0000 ‚Üê Best
...
Epoch 9: Early stopping (5 epochs no improvement)
```

- **Best epoch**: 4/20 (val accuracy = 100%)
- **Training time**: ~150 gi√¢y (CUDA / GPU)
- **Convergence**: Model h·ªôi t·ª• nhanh nh·ªù pre-trained PhoBERT ƒë√£ c√≥ representation t·ªët cho ti·∫øng Vi·ªát

### 6.6 Artifacts ƒë·∫ßu ra

| File | Size | M√¥ t·∫£ |
|---|:---:|---|
| `model.pt` | 540MB | Full model state_dict (PhoBERT + classifier head) |
| `classifier_head.pt` | 34KB | Weights c·ªßa Linear layer + label classes |
| `label_encoder.pkl` | 1.4KB | Mapping gi·ªØa class index v√† t√™n l·ªói |
| `metadata.json` | 2.2KB | Config, metrics, training time, class distribution |

---

## 7. Evaluation Framework

### 7.1 B·ªô d·ªØ li·ªáu ƒë√°nh gi√°

- **149 m·∫´u** (evaluation_dataset.json) ‚Äî ho√†n to√†n t√°ch bi·ªát v·ªõi training data
- **10 classes** √ó ~15 m·∫´u/class (balanced)
- C√°c m·∫´u ƒë∆∞·ª£c thi·∫øt k·∫ø ƒëa d·∫°ng: negation, multi-symptom, edge cases, paraphrase

### 7.2 K·∫øt qu·∫£ so s√°nh (Evaluation Dataset ‚Äî 149 samples)

| Metric | PhoBERT (Fine-tuned) | TF-IDF | PhoBERT (Zero-shot) |
|---|:---:|:---:|:---:|
| **Accuracy** | **89.93%** ‚úÖ | 89.26% | 73.83% |
| **Precision (macro)** | **90.63%** | 90.02% | ‚Äî |
| **Recall (macro)** | **89.86%** | 89.19% | ‚Äî |
| **F1 (macro)** | **89.99%** | 89.31% | 74.02% |
| **Misclassified** | **15/149** | 16/149 | 39/149 |
| **Latency (mean)** | 30.5 ms | 2.9 ms | ‚Äî |
| **Latency (P95)** | 35.7 ms | 3.7 ms | ‚Äî |

### 7.3 Per-class F1-score

| Lo·∫°i l·ªói | PhoBERT (FT) | TF-IDF | Winner |
|---|:---:|:---:|:---:|
| Ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh | 0.93 | **1.00** | TF-IDF |
| Qu√° nhi·ªát | 0.85 | 0.85 | H√≤a |
| H·ªèng b·∫°c ƒë·∫°n / v√≤ng bi | 0.90 | 0.90 | H√≤a |
| Ch√°y cu·ªôn d√¢y / ch√°y motor | 0.84 | **0.90** | TF-IDF |
| S·ª± c·ªë ƒëi·ªán | 0.75 | **0.84** | TF-IDF |
| Qu√° t·∫£i c∆° kh√≠ | 0.93 | 0.93 | H√≤a |
| R√≤ r·ªâ h·ªá th·ªëng | **0.97** | 0.93 | PhoBERT |
| H∆∞ h·ªèng c∆° kh√≠ | **0.94** | 0.81 | PhoBERT |
| √Çm thanh b·∫•t th∆∞·ªùng | **0.93** | 0.90 | PhoBERT |
| Gi·∫£m hi·ªáu su·∫•t | **0.97** | 0.88 | PhoBERT |

### 7.4 Ph√¢n t√≠ch k·∫øt qu·∫£

**PhoBERT v∆∞·ª£t tr·ªôi ·ªü c√°c class**:
- **H∆∞ h·ªèng c∆° kh√≠** (F1: 0.94 vs 0.81): PhoBERT hi·ªÉu ng·ªØ c·∫£nh c∆° kh√≠ t·ªët h∆°n, ph√¢n bi·ªát ƒë∆∞·ª£c c√°c tri·ªáu ch·ª©ng c∆° kh√≠ ph·ª©c t·∫°p
- **Gi·∫£m hi·ªáu su·∫•t** (F1: 0.97 vs 0.88): C√¢u m√¥ t·∫£ gi·∫£m hi·ªáu su·∫•t th∆∞·ªùng d√†i, c√≥ nhi·ªÅu paraphrase ‚Üí PhoBERT semantic matching t·ªët h∆°n
- **R√≤ r·ªâ h·ªá th·ªëng** (F1: 0.97 vs 0.93): PhoBERT nh·∫≠n bi·∫øt ng·ªØ c·∫£nh "r√≤ r·ªâ" trong nhi·ªÅu bi·∫øn th·ªÉ m√¥ t·∫£

**TF-IDF v∆∞·ª£t tr·ªôi ·ªü c√°c class**:
- **Ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh** (F1: 1.00 vs 0.93): TF-IDF d√πng keyword "·ªïn ƒë·ªãnh", "b√¨nh th∆∞·ªùng" r√µ r√†ng; PhoBERT ƒë√¥i khi b·ªã confuse b·ªüi negation ("kh√¥ng n√≥ng" ‚Üí nh·∫ßm th√†nh fault class)
- **S·ª± c·ªë ƒëi·ªán** (F1: 0.84 vs 0.75): T·ª´ v·ª±ng ƒëi·ªán th∆∞·ªùng specific ‚Üí TF-IDF b·∫Øt keyword t·ªët h∆°n

### 7.5 Trade-offs gi·ªØa 2 Engines

| Ti√™u ch√≠ | PhoBERT | TF-IDF |
|---|---|---|
| **Semantic Understanding** | ‚úÖ Hi·ªÉu ng·ªØ nghƒ©a, paraphrase | ‚ùå Ch·ªâ match keyword |
| **T·ªëc ƒë·ªô** | ~30ms/sample | ‚úÖ ~3ms/sample (10x nhanh h∆°n) |
| **Resource** | C·∫ßn GPU/RAM l·ªõn (540MB model) | ‚úÖ R·∫•t nh·∫π (~50KB) |
| **Generalization** | ‚úÖ T·ªët v·ªõi c√¢u ch∆∞a th·∫•y | K√©m v·ªõi c√¢u kh√°c bi·ªát nhi·ªÅu |
| **Interpretability** | ‚ùå Black-box | ‚úÖ Feature importance r√µ |
| **Training Data** | C·∫ßn √≠t h∆°n nh·ªù transfer learning | C·∫ßn nhi·ªÅu data ƒëa d·∫°ng |

---

## 8. Training Pipeline

### 8.1 Data Preparation (`data_preparation.py`)

```mermaid
graph LR
    A["FAULT_REFERENCES<br/>(150 m·∫´u g·ªëc)"] --> D["Combined<br/>Dataset"]
    B["test_dataset.json<br/>(350 m·∫´u)"] --> D
    D --> E["Data Augmentation<br/>(Synonym Replacement)"]
    E --> F["Final Dataset<br/>(1,197 m·∫´u)"]
    F --> G["Train Set<br/>(957 m·∫´u)"]
    F --> H["Val Set<br/>(240 m·∫´u)"]
```

**Data Augmentation Strategy**: Thay th·∫ø t·ª´ ƒë·ªìng nghƒ©a (synonym mapping) ƒë·ªÉ m·ªü r·ªông d·ªØ li·ªáu. V√≠ d·ª•:
- "motor" ‚Üî "ƒë·ªông c∆°" ‚Üî "m√°y"
- "n√≥ng" ‚Üî "ph√°t nhi·ªát" ‚Üî "t·ªèa nhi·ªát"
- "rung" ‚Üî "rung l·∫Øc" ‚Üî "rung ƒë·ªông"

### 8.2 Training TF-IDF (`train_tfidf.py`)

```
python -m backend.training.train_tfidf
```

- TF-IDF Vectorizer ‚Üí Logistic Regression (ho·∫∑c SVM)
- Output: `resources/tfidf/` (vectorizer + classifier + metadata)

### 8.3 Fine-tuning PhoBERT (`train_phobert.py`)

```
python -m backend.training.train_phobert
```

- Load PhoBERT pre-trained ‚Üí Add classification head ‚Üí Fine-tune full model
- AdamW + Linear Warmup + Cosine Decay scheduler
- Early stopping based on validation F1
- Output: `resources/phobert-finetuned/` (model + head + label_encoder + metadata)

---

## 9. API Endpoints

| Method | Path | M√¥ t·∫£ |
|---|---|---|
| `GET` | `/` | Serve giao di·ªán web |
| `POST` | `/analyze` | Ph√¢n t√≠ch NLP (ch·ªçn engine: `phobert` ho·∫∑c `tfidf`) |
| `GET` | `/history` | L·∫•y l·ªãch s·ª≠ ph√¢n t√≠ch (ph√¢n trang) |
| `GET` | `/history/{id}` | Chi ti·∫øt 1 b·∫£n ghi |
| `DELETE` | `/history/{id}` | X√≥a 1 b·∫£n ghi |
| `DELETE` | `/history` | X√≥a to√†n b·ªô l·ªãch s·ª≠ |

---

## 10. Stack c√¥ng ngh·ªá

| Th√†nh ph·∫ßn | C√¥ng ngh·ªá |
|---|---|
| NLP Model (Primary) | **PhoBERT** (`vinai/phobert-base`) + Fine-tuned classifier |
| NLP Model (Secondary) | **TF-IDF** + Logistic Regression (scikit-learn) |
| Backend | **FastAPI** + Uvicorn |
| Database | **SQLite** |
| Frontend | **HTML/CSS/JS** (Single-page) |
| ML Framework | **PyTorch** (CUDA/CPU) |
| Tokenization | **HuggingFace Transformers** |
| Validation | **Pydantic** v2 |
| Evaluation | **scikit-learn** (classification_report, confusion_matrix) |

---

## 11. Lu·ªìng x·ª≠ l√Ω ch√≠nh (Use Case)

```mermaid
sequenceDiagram
    actor User
    participant UI as Frontend (ui.html)
    participant API as FastAPI (app.py)
    participant Factory as Engine Factory
    participant Engine as PhoBERT/TF-IDF Engine
    participant DB as SQLite

    User->>UI: Nh·∫≠p m√¥ t·∫£ thi·∫øt b·ªã<br/>"Motor n√≥ng b·∫•t th∆∞·ªùng, rung m·∫°nh"
    UI->>API: POST /analyze<br/>{equipment, description, engine}
    API->>Factory: get_engine(engine_name)
    Factory->>Engine: analyze(equipment, description)
    Engine->>Engine: 1. Ti·ªÅn x·ª≠ l√Ω
    Engine->>Engine: 2. Tokenization / Vectorization
    Engine->>Engine: 3. Keyword Extraction
    Engine->>Engine: 4. Classification (Softmax / TF-IDF)
    Engine->>Engine: 5. Severity Assessment
    Engine->>Engine: 6. Recommendations
    Engine-->>API: AnalysisResult
    API->>DB: save_analysis()
    DB-->>API: record_id
    API-->>UI: AnalyzeResponse (JSON)
    UI-->>User: Hi·ªÉn th·ªã k·∫øt qu·∫£:<br/>Lo·∫°i l·ªói, M·ª©c ƒë·ªô, Keywords,<br/>Khuy·∫øn ngh·ªã, Pipeline steps
```

---

## 12. ƒê·∫∑c ƒëi·ªÉm n·ªïi b·∫≠t

- **Multi-Engine Architecture**: H·ªó tr·ª£ nhi·ªÅu engine NLP song song, d·ªÖ m·ªü r·ªông th√™m engine m·ªõi (ch·ªâ c·∫ßn implement `BaseNLPEngine`)
- **Fine-tuned PhoBERT Classifier**: Kh√¥ng ch·ªâ d√πng zero-shot m√† fine-tune th√†nh supervised classifier, ƒë·∫°t **89.93% accuracy**
- **Transfer Learning**: T·∫≠n d·ª•ng pre-trained PhoBERT (135M params) ƒë√£ c√≥ ki·∫øn th·ª©c ti·∫øng Vi·ªát, ch·ªâ c·∫ßn ~1,200 m·∫´u ƒë·ªÉ fine-tune
- **Semantic Understanding**: PhoBERT hi·ªÉu ng·ªØ nghƒ©a s√¢u ‚Äî ph√¢n bi·ªát paraphrase, x·ª≠ l√Ω c√¢u ph·ª©c t·∫°p
- **Negation Handling**: Nh·∫≠n bi·∫øt ph·ªß ƒë·ªãnh ("kh√¥ng n√≥ng" ‚â† "n√≥ng") ·ªü c·∫£ 2 level: keyword extraction + classifier
- **Comprehensive Evaluation**: Framework ƒë√°nh gi√° v·ªõi accuracy, precision, recall, F1, confusion matrix, latency benchmark
- **Reproducibility**: Fixed random seed (42), stratified split, l∆∞u metadata ƒë·∫ßy ƒë·ªß cho m·ªói l·∫ßn training
- **Transparency**: Hi·ªÉn th·ªã chi ti·∫øt t·ª´ng b∆∞·ªõc pipeline cho user, bao g·ªìm classification method ƒëang s·ª≠ d·ª•ng

---

## 13. H∆∞·ªõng c·∫£i thi·ªán ti·∫øp theo

1. **TƒÉng d·ªØ li·ªáu training**: Th√™m m·∫´u cho c√°c class y·∫øu (S·ª± c·ªë ƒëi·ªán: F1=0.75, Ch√°y cu·ªôn d√¢y: F1=0.84)
2. **Advanced Augmentation**: Back-translation, contextual word replacement (s·ª≠ d·ª•ng PhoBERT MLM)
3. **Ensemble**: K·∫øt h·ª£p predictions t·ª´ c·∫£ 2 engines (voting, stacking)
4. **Cross-validation**: K-fold CV thay v√¨ single train/val split ƒë·ªÉ ƒë√°nh gi√° ·ªïn ƒë·ªãnh h∆°n
5. **Hyperparameter Tuning**: Grid search / Bayesian optimization cho LR, dropout, batch size
6. **Attention Visualization**: S·ª≠ d·ª•ng attention weights ƒë·ªÉ gi·∫£i th√≠ch prediction (XAI)
