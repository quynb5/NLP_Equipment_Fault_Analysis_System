# üìñ Instructions ‚Äî H∆∞·ªõng d·∫´n ƒë·ªçc t√†i li·ªáu Project

## T·ªïng quan

Project n√†y c√≥ **5 file t√†i li·ªáu `.md`**, m·ªói file ph·ª•c v·ª• m·ªôt m·ª•c ƒë√≠ch kh√°c nhau. D∆∞·ªõi ƒë√¢y l√† h∆∞·ªõng d·∫´n m·ªói file ch·ª©a g√¨, n√™n ƒë·ªçc khi n√†o, v√† c·∫ßn hi·ªÉu nh·ªØng g√¨.

---

## 1. README.md

**M·ª•c ƒë√≠ch**: H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t v√† ch·∫°y project

**N·ªôi dung ch√≠nh**:
- Gi·ªõi thi·ªáu ng·∫Øn g·ªçn project
- H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t m√¥i tr∆∞·ªùng (Python, dependencies)
- C√°ch ch·∫°y server, train model, ch·∫°y evaluation
- API endpoints c∆° b·∫£n

**Khi n√†o ƒë·ªçc**: Khi m·ªõi clone project, c·∫ßn setup v√† ch·∫°y l·∫ßn ƒë·∫ßu

**C·∫ßn hi·ªÉu**:
- [ ] C√°ch c√†i ƒë·∫∑t environment v√† dependencies
- [ ] C√°ch start server (`python main.py`)
- [ ] C√°c API endpoints c√≥ s·∫µn

---

## 2. PROJECT_OVERVIEW.md

**M·ª•c ƒë√≠ch**: T√†i li·ªáu k·ªπ thu·∫≠t t·ªïng quan to√†n b·ªô h·ªá th·ªëng

**N·ªôi dung ch√≠nh**:
- Ki·∫øn tr√∫c Multi-Engine (PhoBERT + TF-IDF)
- C·∫•u tr√∫c th∆∞ m·ª•c chi ti·∫øt
- NLP Pipeline 6 b∆∞·ªõc (Ti·ªÅn x·ª≠ l√Ω ‚Üí Tokenization ‚Üí Keyword ‚Üí Classification ‚Üí Severity ‚Üí Recommendation)
- Dual-mode Classification (Fine-tuned vs Zero-shot)
- B·∫£ng 10 lo·∫°i l·ªói v√† severity
- API endpoints, stack c√¥ng ngh·ªá
- Sequence diagram lu·ªìng x·ª≠ l√Ω

**Khi n√†o ƒë·ªçc**: Khi c·∫ßn hi·ªÉu t·ªïng th·ªÉ h·ªá th·ªëng ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o

**C·∫ßn hi·ªÉu**:
- [ ] Ki·∫øn tr√∫c Multi-Engine v√† Strategy Pattern (BaseNLPEngine ‚Üí PhoBERTEngine, TFIDFEngine)
- [ ] NLP Pipeline 6 b∆∞·ªõc ‚Äî ƒë·∫∑c bi·ªát b∆∞·ªõc 4 (Dual-mode Classification)
- [ ] S·ª± kh√°c bi·ªát gi·ªØa PhoBERT Engine v√† TF-IDF Engine
- [ ] 10 lo·∫°i l·ªói thi·∫øt b·ªã v√† c√°ch t√≠nh severity score
- [ ] Lu·ªìng x·ª≠ l√Ω t·ª´ user input ‚Üí API ‚Üí Engine ‚Üí Database ‚Üí Response

---

## 3. FinetuningModelPhoBert.md

**M·ª•c ƒë√≠ch**: Gi·∫£i th√≠ch k·ªπ thu·∫≠t Fine-tuning PhoBERT ‚Äî ph·∫ßn **h·ªçc thu·∫≠t quan tr·ªçng nh·∫•t**

**N·ªôi dung ch√≠nh**:
- Transfer Learning l√† g√¨ v√† t·∫°i sao d√πng
- Quy tr√¨nh fine-tuning 4 b∆∞·ªõc (Pre-trained ‚Üí Add Head ‚Üí Train ‚Üí Inference)
- Ki·∫øn tr√∫c PhoBERTClassifier (PhoBERT + Dropout + Linear)
- 6 ∆∞u ƒëi·ªÉm c·ªßa Transfer Learning
- So s√°nh v·ªõi c√°c ph∆∞∆°ng ph√°p kh√°c
- K·∫øt qu·∫£ tr∆∞·ªõc/sau fine-tuning
- Thu·∫≠t ng·ªØ h·ªçc thu·∫≠t
- T√†i li·ªáu tham kh·∫£o (papers)

**Khi n√†o ƒë·ªçc**: Khi c·∫ßn b√°o c√°o, thuy·∫øt tr√¨nh v·ªÅ ph∆∞∆°ng ph√°p k·ªπ thu·∫≠t

**C·∫ßn hi·ªÉu**:
- [ ] **Transfer Learning** l√† g√¨ ‚Äî t·∫°i sao kh√¥ng train t·ª´ ƒë·∫ßu
- [ ] **Pre-training vs Fine-tuning** ‚Äî 2 phase ri√™ng bi·ªát
- [ ] **Ki·∫øn tr√∫c model**: PhoBERT (135M params) + Linear head (7,690 params)
- [ ] **[CLS] token pooling** ‚Äî c√°ch l·∫•y embedding ƒë·∫°i di·ªán cho c·∫£ c√¢u
- [ ] **Masked Language Modeling (MLM)** ‚Äî b√†i to√°n pre-training
- [ ] **Contextual embedding** ‚Äî t·∫°i sao "n√≥ng" c√≥ embedding kh√°c nhau t√πy context
- [ ] **∆Øu ƒëi·ªÉm**: Data efficiency, semantic understanding, generalization
- [ ] **Hyperparameters**: AdamW, warmup, cosine decay, early stopping, dropout
- [ ] K·∫øt qu·∫£: 73.83% (zero-shot) ‚Üí **89.93%** (fine-tuned) = +16.1%

---

## 4. BENCHMARK.md

**M·ª•c ƒë√≠ch**: So s√°nh chi ti·∫øt PhoBERT vs TF-IDF ‚Äî b·∫±ng ch·ª©ng th·ª±c nghi·ªám

**N·ªôi dung ch√≠nh**:
- B·∫£ng so s√°nh t·ªïng th·ªÉ (accuracy, F1, precision, recall, latency)
- Per-class F1-score comparison
- **8 m·∫´u c·ª• th·ªÉ** PhoBERT ƒë√∫ng, TF-IDF sai ‚Äî ph√¢n t√≠ch chi ti·∫øt t·∫°i sao
- **7 m·∫´u c·ª• th·ªÉ** TF-IDF ƒë√∫ng, PhoBERT sai ‚Äî ph√¢n t√≠ch ƒëi·ªÉm y·∫øu
- **5 Pattern** gi·∫£i th√≠ch TF-IDF k√©m h∆°n
- Trade-offs gi·ªØa 2 engines
- **5 l√Ω do thuy·∫øt ph·ª•c** n√™n ch·ªçn PhoBERT thay v√¨ TF-IDF

**Khi n√†o ƒë·ªçc**: Khi c·∫ßn ch·ª©ng minh PhoBERT t·ªët h∆°n TF-IDF cho b√°o c√°o

**C·∫ßn hi·ªÉu**:
- [ ] PhoBERT (89.93%) vs TF-IDF (89.26%) ‚Äî PhoBERT th·∫Øng overall
- [ ] **5 Pattern TF-IDF k√©m**:
  1. Root Cause vs Symptom Confusion ‚Äî TF-IDF b·∫Øt tri·ªáu ch·ª©ng ph·ª•, b·ªè qua root cause
  2. Multi-symptom Keyword Dominance ‚Äî TF-IDF b·ªã keyword m·∫°nh nh·∫•t chi ph·ªëi
  3. Paraphrasing ‚Äî TF-IDF fail khi d√πng c√°ch di·ªÖn ƒë·∫°t kh√°c
  4. Fine-grained vs Coarse Classification ‚Äî TF-IDF default class chung chung
  5. Confidence Gap ‚Äî PhoBERT t·ª± tin h∆°n (0.87 vs 0.26)
- [ ] **5 l√Ω do ch·ªçn PhoBERT** (d√π accuracy g·∫ßn nhau):
  1. Ti·ªÅm nƒÉng scale ‚Äî th√™m data PhoBERT c·∫£i thi·ªán m·∫°nh, TF-IDF saturate
  2. Semantic understanding ‚Äî x·ª≠ l√Ω real-world input ƒëa d·∫°ng
  3. Root cause detection ‚Äî ph√¢n lo·∫°i ƒë√∫ng nguy√™n nh√¢n g·ªëc
  4. Confidence calibration ‚Äî tin c·∫≠y h∆°n khi deploy production
  5. Multi-task extensibility ‚Äî m·ªü r·ªông cho NER, QA, sentiment...
- [ ] **PhoBERT y·∫øu ·ªü ƒë√¢u**: Nh·∫≠n di·ªán "Ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh" (F1=0.93 vs TF-IDF 1.00)
- [ ] **Trade-offs**: PhoBERT ch·∫≠m h∆°n 10x (30ms vs 3ms), n·∫∑ng h∆°n 10,000x (540MB vs 50KB)

---

## 5. EVALUATION_REPORT.md

**M·ª•c ƒë√≠ch**: B√°o c√°o k·∫øt qu·∫£ evaluation ch√≠nh th·ª©c

**N·ªôi dung ch√≠nh**:
- K·∫øt qu·∫£ evaluation tr√™n b·ªô 149 m·∫´u test
- Confusion matrix
- Per-class metrics
- Danh s√°ch m·∫´u b·ªã misclassified

**Khi n√†o ƒë·ªçc**: Khi c·∫ßn s·ªë li·ªáu ch√≠nh x√°c ƒë·ªÉ tr√≠ch d·∫´n

**C·∫ßn hi·ªÉu**:
- [ ] C√°ch ƒë·ªçc confusion matrix
- [ ] √ù nghƒ©a c√°c metrics: Accuracy, Precision, Recall, F1-score
- [ ] Macro average vs Weighted average

---

## Th·ª© t·ª± ƒë·ªçc ƒë∆∞·ª£c khuy·∫øn ngh·ªã

```
1. README.md              ‚Üê Setup & ch·∫°y project
2. PROJECT_OVERVIEW.md    ‚Üê Hi·ªÉu t·ªïng th·ªÉ h·ªá th·ªëng
3. FinetuningModelPhoBert.md  ‚Üê Hi·ªÉu k·ªπ thu·∫≠t fine-tuning (h·ªçc thu·∫≠t)
4. BENCHMARK.md           ‚Üê B·∫±ng ch·ª©ng PhoBERT > TF-IDF
5. EVALUATION_REPORT.md   ‚Üê S·ªë li·ªáu ch√≠nh x√°c
```

---

## T√≥m t·∫Øt nhanh ‚Äî Nh·ªØng ƒëi·ªÉm quan tr·ªçng nh·∫•t c·∫ßn n·∫Øm

| C√¢u h·ªèi | Tr·∫£ l·ªùi |
|---|---|
| **H·ªá th·ªëng l√†m g√¨?** | Ph√¢n lo·∫°i l·ªói thi·∫øt b·ªã CN t·ª´ m√¥ t·∫£ ti·∫øng Vi·ªát (10 classes) |
| **D√πng k·ªπ thu·∫≠t g√¨?** | Transfer Learning ‚Äî Fine-tuning PhoBERT |
| **PhoBERT l√† g√¨?** | Pre-trained Language Model cho ti·∫øng Vi·ªát (VinAI, 135M params) |
| **Fine-tuning l√† g√¨?** | Tinh ch·ªânh model pre-trained cho task c·ª• th·ªÉ b·∫±ng d·ªØ li·ªáu √≠t |
| **Accuracy bao nhi√™u?** | PhoBERT: 89.93%, TF-IDF: 89.26% |
| **T·∫°i sao PhoBERT t·ªët h∆°n?** | Hi·ªÉu ng·ªØ nghƒ©a, context, paraphrase ‚Äî kh√¥ng ch·ªâ match keyword |
| **T·∫°i sao TF-IDF v·∫´n c·∫ßn?** | Nhanh 10x, nh·∫π 10,000x, t·ªët cho baseline comparison |
| **D·ªØ li·ªáu c·∫ßn bao nhi√™u?** | Ch·ªâ 1,197 m·∫´u (nh·ªù transfer learning) |
