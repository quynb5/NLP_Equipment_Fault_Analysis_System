"""
PhoBERT Engine - Vietnamese Industrial Equipment Fault Analysis
===============================================================
Pipeline: Vietnamese text ‚Üí Preprocessing ‚Üí PhoBERT Tokenization
           ‚Üí PhoBERT Encoding ‚Üí Fault Classification
           ‚Üí Severity Scoring ‚Üí Recommendation Generation

H·ªó tr·ª£ 2 ch·∫ø ƒë·ªô:
  1. Fine-tuned Classifier (n·∫øu c√≥ model ƒë√£ train)
  2. Zero-shot Cosine Similarity (fallback)
"""

import re
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""  # Force CPU mode

import time
import unicodedata
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer

from backend.core.base_engine import BaseNLPEngine, AnalysisResult


# ============================================================
# 1. PhoBERT MODEL LOADER
# ============================================================

_HF_MODEL_NAME = "vinai/phobert-base"

import pathlib as _pathlib
_MODEL_PATH = str(_pathlib.Path(__file__).resolve().parent.parent / "resources" / "phobert-base")


def _load_model():
    """Load PhoBERT t·ª´ local path, n·∫øu kh√¥ng c√≥ th√¨ download t·ª´ HuggingFace."""
    model_dir = _pathlib.Path(_MODEL_PATH)

    if model_dir.exists() and any(model_dir.iterdir()):
        # Load t·ª´ local path
        print(f"üîÑ ƒêang t·∫£i PhoBERT t·ª´ local: {_MODEL_PATH}")
        try:
            tokenizer = AutoTokenizer.from_pretrained(_MODEL_PATH)
            model = AutoModel.from_pretrained(_MODEL_PATH)
            print("‚úÖ Load t·ª´ local th√†nh c√¥ng")
            return tokenizer, model
        except Exception as e:
            print(f"‚ö†Ô∏è Load t·ª´ local th·∫•t b·∫°i: {e}")
            print("üîÑ S·∫Ω download l·∫°i t·ª´ HuggingFace...")

    # Download t·ª´ HuggingFace v√† l∆∞u v√†o _MODEL_PATH
    print(f"üîÑ ƒêang download PhoBERT ({_HF_MODEL_NAME}) t·ª´ HuggingFace...")
    model_dir.mkdir(parents=True, exist_ok=True)
    tokenizer = AutoTokenizer.from_pretrained(_HF_MODEL_NAME)
    model = AutoModel.from_pretrained(_HF_MODEL_NAME)
    tokenizer.save_pretrained(_MODEL_PATH)
    model.save_pretrained(_MODEL_PATH)
    print(f"‚úÖ Download v√† l∆∞u th√†nh c√¥ng t·∫°i: {_MODEL_PATH}")
    return tokenizer, model


_tokenizer, _model = _load_model()
_model.eval()  # Ch·∫ø ƒë·ªô inference

# Ch·ªçn device
_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
_model.to(_device)

print(f"‚úÖ PhoBERT ready on {_device}")


# ============================================================
# 1b. FINE-TUNED CLASSIFIER (optional)
# ============================================================

class _PhoBERTClassifier(nn.Module):
    """PhoBERT + Linear classification head."""
    def __init__(self, phobert_model, num_classes=10, dropout=0.3):
        super().__init__()
        self.phobert = phobert_model
        hidden_size = self.phobert.config.hidden_size
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask)
        cls_output = outputs.last_hidden_state[:, 0, :]
        cls_output = self.dropout(cls_output)
        logits = self.classifier(cls_output)
        return logits


_FINETUNED_DIR = _pathlib.Path(__file__).resolve().parent.parent / "resources" / "phobert-finetuned"
_finetuned_model = None
_finetuned_labels = None


def _load_finetuned_model():
    """Load fine-tuned PhoBERT classifier n·∫øu c√≥."""
    global _finetuned_model, _finetuned_labels

    model_path = _FINETUNED_DIR / "model.pt"
    head_path = _FINETUNED_DIR / "classifier_head.pt"

    if not model_path.exists():
        print("‚ÑπÔ∏è  Fine-tuned model not found ‚Üí using zero-shot similarity")
        return

    try:
        head_info = torch.load(head_path, map_location="cpu", weights_only=False)
        num_classes = head_info["num_classes"]
        dropout_p = head_info.get("dropout_p", 0.3)
        _finetuned_labels = head_info["label_classes"]

        classifier = _PhoBERTClassifier(
            phobert_model=_model,
            num_classes=num_classes,
            dropout=dropout_p,
        )

        state_dict = torch.load(model_path, map_location=_device, weights_only=False)
        classifier.load_state_dict(state_dict, strict=False)
        classifier.to(_device)
        classifier.eval()

        _finetuned_model = classifier
        print(f"‚úÖ Fine-tuned PhoBERT classifier loaded ({num_classes} classes)")
        print(f"   Labels: {_finetuned_labels}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to load fine-tuned model: {e}")
        print("   Falling back to zero-shot similarity")


_load_finetuned_model()


# ============================================================
# 2. FAULT REFERENCE DATABASE
# ============================================================

# C√°c m·∫´u m√¥ t·∫£ l·ªói tham chi·∫øu ‚Äî PhoBERT s·∫Ω so s√°nh semantic similarity
FAULT_REFERENCES = {
    "B√¨nh th∆∞·ªùng": {
        "samples": [
            "thi·∫øt b·ªã ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng ·ªïn ƒë·ªãnh",
            "m√°y ch·∫°y t·ªët kh√¥ng c√≥ v·∫•n ƒë·ªÅ g√¨",
            "ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng kh√¥ng c√≥ ti·∫øng ·ªìn l·∫°",
            "m·ªçi th·ª© ·ªïn ƒë·ªãnh kh√¥ng ph√°t hi·ªán b·∫•t th∆∞·ªùng",
            "thi·∫øt b·ªã v·∫≠n h√†nh t·ªët kh√¥ng rung kh√¥ng n√≥ng",
            "m√°y ho·∫°t ƒë·ªông √™m kh√¥ng c√≥ m√πi l·∫°",
            "t√¨nh tr·∫°ng t·ªët nhi·ªát ƒë·ªô b√¨nh th∆∞·ªùng",
            "motor ch·∫°y √™m √°i kh√¥ng c√≥ ti·∫øng ƒë·ªông l·∫°",
            "ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng ch·∫°y √™m √°i",
            "m√°y ch·∫°y √™m kh√¥ng rung kh√¥ng n√≥ng kh√¥ng m√πi",
            "thi·∫øt b·ªã ch·∫°y ·ªïn ƒë·ªãnh kh√¥ng c√≥ b·∫•t th∆∞·ªùng g√¨",
            "ho·∫°t ƒë·ªông t·ªët kh√¥ng c√≥ s·ª± c·ªë",
            "v·∫≠n h√†nh b√¨nh th∆∞·ªùng kh√¥ng ph√°t hi·ªán h∆∞ h·ªèng",
            "m√°y ho·∫°t ƒë·ªông t·ªët kh√¥ng c·∫ßn b·∫£o tr√¨",
            "t·∫•t c·∫£ ch·ªâ s·ªë b√¨nh th∆∞·ªùng thi·∫øt b·ªã ·ªïn ƒë·ªãnh",
            # --- Negation & new patterns ---
            "motor v·∫≠n h√†nh tr∆°n tru kh√¥ng c√≥ ti·∫øng ·ªìn b·∫•t th∆∞·ªùng nhi·ªát ƒë·ªô trong ng∆∞·ª°ng",
            "b∆°m n∆∞·ªõc ch·∫°y ƒë·ªÅu √°p su·∫•t ·ªïn ƒë·ªãnh kh√¥ng rung kh√¥ng n√≥ng",
            "qu·∫°t c√¥ng nghi·ªáp ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng kh√¥ng ph√°t hi·ªán d·∫•u hi·ªáu b·∫•t th∆∞·ªùng",
            "h·ªá th·ªëng b∆°m th·ªßy l·ª±c v·∫≠n h√†nh ·ªïn ƒë·ªãnh su·ªët ca kh√¥ng r√≤ r·ªâ kh√¥ng ti·∫øng k√™u",
            "m√°y n√©n kh√≠ ch·∫°y t·ªët kh√¥ng n√≥ng b·∫•t th∆∞·ªùng c√¥ng su·∫•t ƒë·∫°t y√™u c·∫ßu",
            "thi·∫øt b·ªã kh√¥ng c√≥ v·∫•n ƒë·ªÅ g√¨ th√¥ng s·ªë n·∫±m trong gi·ªõi h·∫°n an to√†n",
            "motor ch·∫°y m∆∞·ª£t m√† kh√¥ng rung l·∫Øc kh√¥ng m√πi kh√©t nhi·ªát ƒë·ªô b√¨nh th∆∞·ªùng",
            "bƒÉng t·∫£i v·∫≠n h√†nh ƒë√∫ng t·ªëc ƒë·ªô thi·∫øt k·∫ø kh√¥ng tr∆∞·ª£t kh√¥ng k·∫πt",
            "h·ªá th·ªëng ch·∫°y √™m nhi·ªát ƒë·ªô ·ªïn ƒë·ªãnh kh√¥ng rung kh√¥ng c√≥ b·∫•t th∆∞·ªùng",
            "thi·∫øt b·ªã ch·∫°y ·ªïn ƒë·ªãnh su·ªët ca l√†m vi·ªác kh√¥ng c√≥ b·∫•t th∆∞·ªùng g√¨",
        ],
        "severity_base": 0.0,
        "is_normal": True,
        "description": "Thi·∫øt b·ªã ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng, kh√¥ng ph√°t hi·ªán b·∫•t th∆∞·ªùng",
    },
    "Qu√° nhi·ªát": {
        "samples": [
            "thi·∫øt b·ªã n√≥ng b·∫•t th∆∞·ªùng nhi·ªát ƒë·ªô r·∫•t cao",
            "motor qu√° n√≥ng t·ªèa nhi·ªát m·∫°nh ch√°y tay",
            "nhi·ªát ƒë·ªô tƒÉng cao b·∫•t th∆∞·ªùng b·ªèng tay n√≥ng ran",
            "thi·∫øt b·ªã b·ªëc h∆°i n√≥ng kh√≥i nhi·ªát tƒÉng",
            "v·ªè m√°y n√≥ng ch·∫£y qu√° nhi·ªát nghi√™m tr·ªçng",
            "m√°y n√≥ng ran s·ªù v√†o b·ªèng da nhi·ªát cao",
            "nhi·ªát t·ªèa ra r·∫•t m·∫°nh thi·∫øt b·ªã qu√° n√≥ng",
            "v·ªè motor n√≥ng d·ªØ d·ªôi nhi·ªát ƒë·ªô v∆∞·ª£t ng∆∞·ª°ng",
            "b∆°m th·ªßy l·ª±c n√≥ng b·∫•t th∆∞·ªùng ch·∫°m b·ªèng tay",
            "n√≥ng h·∫ßm h·∫≠p nhi·ªát l∆∞·ª£ng t·ªèa l·ªõn b·∫•t th∆∞·ªùng",
            "nhi·ªát tƒÉng li√™n t·ª•c kh√¥ng gi·∫£m n√≥ng b·ªëc h∆°i",
            "c·∫£m bi·∫øn nhi·ªát b√°o v∆∞·ª£t ng∆∞·ª°ng thi·∫øt b·ªã qu√° n√≥ng",
            "nhi·ªát ƒë·ªô cao b·∫•t th∆∞·ªùng n√≥ng h∆°n b√¨nh th∆∞·ªùng r·∫•t nhi·ªÅu",
            "thi·∫øt b·ªã ph√°t nhi·ªát m·∫°nh qu√° m·ª©c cho ph√©p",
            "motor n√≥ng li√™n t·ª•c kh√¥ng h·∫° nhi·ªát d√π gi·∫£m t·∫£i",
            # --- New patterns ---
            "v·ªè motor n√≥ng h∆°n b√¨nh th∆∞·ªùng r·∫•t nhi·ªÅu khi ch·∫°m v√†o",
            "nhi·ªát ƒë·ªô b·ªÅ m·∫∑t thi·∫øt b·ªã v∆∞·ª£t ng∆∞·ª°ng c·∫£nh b√°o sensor b√°o qu√° nhi·ªát",
            "motor ph√°t nhi·ªát li√™n t·ª•c d√π t·∫£i nh·∫π h·ªá th·ªëng l√†m m√°t kh√¥ng hi·ªáu qu·∫£",
            "b∆°m n√≥ng ran kh√¥ng ch·∫°m ƒë∆∞·ª£c nhi·ªát k·∫ø ƒëo v∆∞·ª£t 90 ƒë·ªô",
            "nhi·ªát t·ªèa ra t·ª´ thi·∫øt b·ªã l·ªõn b·∫•t th∆∞·ªùng d·∫ßu b√¥i tr∆°n b·ªã lo√£ng",
            "thi·∫øt b·ªã n√≥ng b·ª©c x·∫° qu·∫°t t·∫£n nhi·ªát ch·∫°y h·∫øt c√¥ng su·∫•t kh√¥ng gi·∫£m nhi·ªát",
            "th√¢n m√°y n√≥ng h∆°n m·ª©c cho ph√©p c·∫£m bi·∫øn nhi·ªát li√™n t·ª•c c·∫£nh b√°o",
            "m√°y n√©n n√≥ng qu√° h∆°i n√≥ng b·ªëc l√™n m·∫°nh nhi·ªát tƒÉng b·∫•t th∆∞·ªùng",
            "motor qu√° n√≥ng n√≥ng b·∫•t th∆∞·ªùng so v·ªõi m·ªçi khi nhi·ªát cao",
            "m√°y b·ªëc h∆°i n√≥ng nhi·ªát tƒÉng li√™n t·ª•c kh√¥ng gi·∫£m qu√° nhi·ªát",
        ],
        "severity_base": 0.7,
        "description": "Thi·∫øt b·ªã ho·∫°t ƒë·ªông ·ªü nhi·ªát ƒë·ªô cao b·∫•t th∆∞·ªùng",
    },
    "H·ªèng b·∫°c ƒë·∫°n / v√≤ng bi": {
        "samples": [
            "rung m·∫°nh k√®m ti·∫øng kim lo·∫°i va ch·∫°m",
            "ti·∫øng k√™u l·∫° rung l·∫Øc m·∫°nh ti·∫øng c·ªç s√°t",
            "rung b·∫•t th∆∞·ªùng ti·∫øng r√≠t cao ti·∫øng kim lo·∫°i",
            "rung ƒë·ªông m·∫°nh k√®m ti·∫øng va ƒë·∫≠p b·∫°c ƒë·∫°n v√≤ng bi",
            "ti·∫øng l√°ch c√°ch rung li√™n t·ª•c gi·∫≠t c·ª•c",
            "rung l·∫Øc k√®m ti·∫øng c·ªç kim lo·∫°i b√™n trong motor",
            "ti·∫øng k√™u r√≠t t·ª´ v√≤ng bi rung l·∫Øc m·∫°nh",
            "tr·ª•c m√°y rung gi·∫≠t c·ª•c ti·∫øng k√™u l·∫° li√™n t·ª•c",
            "ti·∫øng kim lo·∫°i c·ªç s√°t m·∫°nh rung gi·∫≠t li√™n t·ª•c",
            "b·∫°c ƒë·∫°n ph√°t ti·∫øng k√™u r√≠t rung l·∫Øc tƒÉng d·∫ßn",
            "·ªï bi k√™u to rung l·∫Øc r√µ r·ªát khi v·∫≠n h√†nh",
            "ti·∫øng c·ªç s√°t kim lo·∫°i c·∫£m nh·∫≠n rung m·∫°nh ·ªü tr·ª•c",
            "motor gi·∫≠t c·ª•c li√™n t·ª•c ti·∫øng l√°ch c√°ch b·∫°c ƒë·∫°n",
            "rung d·ªØ d·ªôi k√®m √¢m thanh kim lo·∫°i va ch·∫°m ·ªï bi",
            "ph√°t hi·ªán rung b·∫•t th∆∞·ªùng ti·∫øng r√≠t cao t·ª´ v√≤ng bi",
            # --- New patterns (negation + technical) ---
            "motor kh√¥ng n√≥ng nh∆∞ng rung l·∫Øc m·∫°nh ph√°t ti·∫øng k√™u kim lo·∫°i",
            "nghe ti·∫øng r√≠t t·ª´ v·ªã tr√≠ b·∫°c ƒë·∫°n motor rung tƒÉng d·∫ßn theo th·ªùi gian",
            "v√≤ng bi ph√≠a sau motor c√≥ d·∫•u hi·ªáu m√≤n ti·∫øng k√™u r√® khi quay",
            "tr·ª•c motor l·ªèng l·∫ªo l·∫Øc qua l·∫°i khi quay nghi ng·ªù b·∫°c ƒë·∫°n m√≤n",
            "rung t·∫ßn s·ªë cao ph√°t ra t·ª´ ·ªï tr·ª•c d·∫ßu m·ª° b√¥i tr∆°n b·ªã bi·∫øn m√†u",
            "motor rung k√®m ti·∫øng l·∫°ch c·∫°ch nh·ªãp ƒë·ªÅu ƒë·∫∑n khi quay ch·∫≠m",
            "·ªï bi ph√°t ra ti·∫øng √π li√™n t·ª•c b·ªÅ m·∫∑t tr·ª•c b·ªã x∆∞·ªõc",
            "kh√¥ng n√≥ng kh√¥ng kh√©t ch·ªâ rung m·∫°nh k√®m ti·∫øng kim lo·∫°i ·ªï bi",
            "motor rung l·∫Øc b·∫•t th∆∞·ªùng ·ªï tr·ª•c ph√°t ti·∫øng k√™u nghi b·∫°c ƒë·∫°n h·ªèng",
            "ti·∫øng r√≠t cao t·ª´ b·∫°c ƒë·∫°n k√®m rung khi t·∫£i tƒÉng",
        ],
        "severity_base": 0.75,
        "description": "Rung ƒë·ªông + ti·∫øng kim lo·∫°i ‚Äî nghi ng·ªù h·ªèng b·∫°c ƒë·∫°n ho·∫∑c v√≤ng bi",
    },
    "Ch√°y cu·ªôn d√¢y / ch√°y motor": {
        "samples": [
            "m√πi kh√©t ch√°y k√®m nhi·ªát ƒë·ªô r·∫•t cao b·ªëc kh√≥i",
            "m√πi ch√°y m√πi nh·ª±a ch√°y n√≥ng b·∫•t th∆∞·ªùng b·ªëc kh√≥i",
            "kh√©t m√πi d·∫ßu ch√°y qu√° nhi·ªát nghi√™m tr·ªçng kh√≥i",
            "motor ch√°y m√πi kh√©t n√≥ng ch·∫£y tia l·ª≠a ƒëi·ªán",
            "cu·ªôn d√¢y ch√°y m√πi cao su ch√°y b·ªëc kh√≥i nhi·ªát cao",
            "b·ªëc kh√≥i ƒëen m√πi ch√°y kh√©t n·ªìng n·∫∑c t·ª´ motor",
            "m√πi kh√©t ch√°y r·∫•t n·∫∑ng k√®m kh√≥i b·ªëc m·∫°nh",
            "kh√≥i b·ªëc ra t·ª´ cu·ªôn d√¢y motor m√πi ch√°y n·ªìng",
            "motor ph√°t m√πi kh√©t d·ªØ d·ªôi b·ªëc kh√≥i n√≥ng ch·∫£y",
            "ph√°t hi·ªán kh√≥i m√πi ch√°y tia l·ª≠a t·ª´ cu·ªôn d√¢y",
            "m√πi d·∫ßu ch√°y kh√©t t·ª´ motor b·ªëc kh√≥i ƒëen n√≥ng cao",
            "motor b·ªëc kh√≥i m√πi ch√°y nh·ª±a n√≥ng kh√¥ng ch·∫°m ƒë∆∞·ª£c",
            "cu·ªôn d√¢y motor ch√°y ƒëen m√πi kh√©t b·ªëc kh√≥i li√™n t·ª•c",
            "thi·∫øt b·ªã b·ªëc kh√≥i m√πi kh√©t ch√°y c·ª±c k·ª≥ n√≥ng",
            "m√πi ch√°y kh√©t n·ªìng n·∫∑c motor n√≥ng ch·∫£y b·ªëc kh√≥i",
            # --- New patterns (negation + technical) ---
            "thi·∫øt b·ªã kh√¥ng rung nh∆∞ng b·ªëc m√πi kh√©t ch√°y d·ªØ d·ªôi k√®m kh√≥i ƒëen",
            "cu·ªôn d√¢y stator b·ªã ng·∫Øn m·∫°ch b·ªëc kh√≥i m√πi nh·ª±a n√≥ng ch·∫£y",
            "motor ph√°t tia l·ª≠a b√™n trong m√πi ch√°y kh√©t n·ªìng n·∫∑c lan kh·∫Øp ph√≤ng",
            "l·ªõp c√°ch ƒëi·ªán cu·ªôn d√¢y b·ªã ch·∫£y motor b·ªëc kh√≥i tr·∫Øng d·ª´ng quay",
            "ph√°t hi·ªán v√°ng ch√°y ƒëen tr√™n cu·ªôn d√¢y khi th√°o motor ki·ªÉm tra",
            "motor b·∫•t ng·ªù d·ª´ng k√®m m√πi ch√°y ki·ªÉm tra th·∫•y d√¢y qu·∫•n ƒë·ª©t ch√°y",
            "kh√¥ng rung kh√¥ng ·ªìn nh∆∞ng m√πi kh√©t d·ªØ d·ªôi b·ªëc kh√≥i t·ª´ motor",
            "cu·ªôn d√¢y motor b·ªã ƒëo·∫£n m·∫°ch ch√°y ƒëen m√πi kh√©t b·ªëc kh√≥i n·∫∑ng",
            "motor ch√°y cu·ªôn d√¢y stator b·ªã ng·∫Øn m·∫°ch ch·∫≠p gi·ªØa c√°c pha",
            "m√πi ch√°y kh√©t n·∫∑ng n·ªÅ motor d·ª´ng ƒë·ªôt ng·ªôt b·ªëc kh√≥i ƒëen",
        ],
        "severity_base": 0.9,
        "description": "Qu√° nhi·ªát k·∫øt h·ª£p m√πi ch√°y ‚Äî ch√°y cu·ªôn d√¢y ho·∫∑c ch√°y motor",
    },
    "S·ª± c·ªë ƒëi·ªán": {
        "samples": [
            "d√≤ng ƒëi·ªán tƒÉng ƒë·ªôt ng·ªôt ch·∫≠p m·∫°ch ph√≥ng ƒëi·ªán",
            "tia l·ª≠a ƒëi·ªán r√≤ ƒëi·ªán ch·∫≠p ƒëi·ªán",
            "d√≤ng ƒëi·ªán dao ƒë·ªông b·∫•t th∆∞·ªùng qu√° t·∫£i ch√°y c·∫ßu ch√¨",
            "m·∫•t pha l·ªách pha s·ª•t √°p d√≤ng ƒëi·ªán b·∫•t th∆∞·ªùng",
            "ƒëi·ªán gi·∫≠t r√≤ ƒëi·ªán nguy hi·ªÉm ch·∫≠p m·∫°ch",
            "ch·∫≠p ƒëi·ªán ch√°y c·∫ßu ch√¨ s·ª•t √°p nghi√™m tr·ªçng",
            "r√≤ ƒëi·ªán ra v·ªè thi·∫øt b·ªã ch·∫°m v√†o b·ªã gi·∫≠t",
            "ch·∫≠p m·∫°ch t·ªß ƒëi·ªán ƒëi·ªÅu khi·ªÉn c·∫ßu ch√¨ n·ªï",
            "tia l·ª≠a ƒëi·ªán ph√≥ng ra t·ª´ ƒë·∫ßu n·ªëi nguy hi·ªÉm",
            "d√≤ng ƒëi·ªán tƒÉng ƒë·ªôt ng·ªôt v∆∞·ª£t gi·ªõi h·∫°n cho ph√©p",
            "s·ª•t √°p tr·∫ßm tr·ªçng thi·∫øt b·ªã ch·∫°y y·∫øu ƒëi·ªán kh√¥ng ·ªïn",
            "l·ªách pha g√¢y rung motor d√≤ng ƒëi·ªán b·∫•t th∆∞·ªùng",
            "r√≤ ƒëi·ªán nghi√™m tr·ªçng v·ªè m√°y ƒëi·ªán gi·∫≠t",
            "relay b·∫£o v·ªá nh·∫£y ng·∫Øt li√™n t·ª•c ch·∫≠p ƒëi·ªán",
            "d√≤ng ƒëi·ªán qu√° t·∫£i g√¢y ch√°y c·∫ßu ch√¨ b·∫£o v·ªá",
            # --- New patterns (negation + technical terms) ---
            "thi·∫øt b·ªã kh√¥ng n√≥ng kh√¥ng rung nh∆∞ng d√≤ng ƒëi·ªán dao ƒë·ªông b·∫•t th∆∞·ªùng li√™n t·ª•c",
            "MCB b·∫£o v·ªá nh·∫£y li√™n t·ª•c khi kh·ªüi ƒë·ªông motor nghi ng·ªù ch·∫°m mass",
            "d√¢y c√°p ngu·ªìn b·ªã ch·∫£y v·ªè nh·ª±a do qu√° d√≤ng ƒë·∫ßu n·ªëi b·ªã ƒëen oxy h√≥a",
            "bi·∫øn t·∫ßn b√°o l·ªói qu√° d√≤ng pha R motor kh·ªüi ƒë·ªông r·ªìi t·ª± ng·∫Øt",
            "aptomat ch·ªëng r√≤ nh·∫£y khi motor ch·∫°y nghi ng·ªù r√≤ ƒëi·ªán ra th√¢n v·ªè",
            "motor b·ªã m·∫•t pha g√¢y rung b·∫•t th∆∞·ªùng c·∫ßu ch√¨ m·ªôt pha b·ªã ƒë·ª©t",
            "relay nhi·ªát t√°c ƒë·ªông ng·∫Øt motor li√™n t·ª•c d√π t·∫£i kh√¥ng ƒë·ªïi",
            "ƒë·∫ßu n·ªëi c√°p b·ªã h·ªì quang ƒëi·ªán m√πi ozone tia l·ª≠a ph√≥ng ra",
            "kh√¥ng n√≥ng kh√¥ng rung ch·ªâ c√≥ d√≤ng ƒëi·ªán b·∫•t ·ªïn MCB nh·∫£y li√™n t·ª•c",
            "bi·∫øn t·∫ßn b√°o l·ªói s·ª± c·ªë ƒëi·ªán motor kh√¥ng kh·ªüi ƒë·ªông ƒë∆∞·ª£c",
        ],
        "severity_base": 0.85,
        "description": "V·∫•n ƒë·ªÅ h·ªá th·ªëng ƒëi·ªán ‚Äî ch·∫≠p, r√≤, qu√° t·∫£i",
    },
    "Qu√° t·∫£i c∆° kh√≠": {
        "samples": [
            "n√≥ng b·∫•t th∆∞·ªùng k√®m rung m·∫°nh qu√° t·∫£i",
            "nhi·ªát ƒë·ªô cao rung ƒë·ªông m·∫°nh thi·∫øt b·ªã qu√° t·∫£i",
            "qu√° n√≥ng rung l·∫Øc m·∫°nh d√¢y ƒëai cƒÉng qu√° t·∫£i",
            "motor n√≥ng rung m·∫°nh ch·∫°y ch·∫≠m c√¥ng su·∫•t gi·∫£m",
            "thi·∫øt b·ªã qu√° t·∫£i n√≥ng rung gi·∫≠t k·∫πt",
            "motor n√≥ng rung m·∫°nh ch·∫°y y·∫øu qu√° t·∫£i li√™n t·ª•c",
            "thi·∫øt b·ªã qu√° t·∫£i n√≥ng k√®m rung l·∫Øc m·∫°nh gi·∫≠t c·ª•c",
            "n√≥ng b·∫•t th∆∞·ªùng k√®m rung do ch·∫°y qu√° c√¥ng su·∫•t",
            "n√≥ng rung m·∫°nh d√¢y ƒëai cƒÉng qu√° m·ª©c do qu√° t·∫£i",
            "qu√° t·∫£i li√™n t·ª•c g√¢y n√≥ng motor rung m·∫°nh gi·∫£m c√¥ng su·∫•t",
            "m√°y n√≥ng ch·∫°y ch·∫≠m h·∫≥n do qu√° t·∫£i c∆° kh√≠ nghi√™m tr·ªçng",
            "rung m·∫°nh k√®m n√≥ng thi·∫øt b·ªã b·ªã k·∫πt do qu√° t·∫£i",
            "motor n√≥ng ran rung l·∫Øc t·∫£i v∆∞·ª£t th√¥ng s·ªë thi·∫øt k·∫ø",
            "m√°y n√©n qu√° t·∫£i n√≥ng rung m·∫°nh c√¥ng su·∫•t s·ª•t gi·∫£m",
            "t·∫£i qu√° n·∫∑ng khi·∫øn motor n√≥ng rung ch·∫°y ch·∫≠m l·∫°i",
            # --- New patterns (negation + cross-symptom) ---
            "motor kh√¥ng kh√©t kh√¥ng ch√°y nh∆∞ng n√≥ng k√®m rung do ch·∫°y v∆∞·ª£t c√¥ng su·∫•t",
            "thi·∫øt b·ªã k·∫πt t·∫£i n·∫∑ng ampe k·∫ø ch·ªâ v∆∞·ª£t ƒë·ªãnh m·ª©c motor r√≠t ch·∫°y ch·∫≠m",
            "bƒÉng t·∫£i b·ªã qu√° t·∫£i do h√†ng h√≥a ch·∫•t nhi·ªÅu g√¢y motor cƒÉng d√¢y ƒëai",
            "motor k√©o t·∫£i qu√° n·∫∑ng d√¢y ƒëai tr∆∞·ª£t li√™n t·ª•c ph√°t m√πi cao su",
            "m√°y b∆°m b·ªã qu√° t·∫£i do van ƒë·∫ßu ra ƒë√≥ng √°p su·∫•t tƒÉng motor rung gi·∫≠t",
            "c√¥ng su·∫•t y√™u c·∫ßu v∆∞·ª£t xa th√¥ng s·ªë thi·∫øt k·∫ø motor n√≥ng k√®m ti·∫øng r√≠t",
            "t·∫£i c∆° kh√≠ qu√° l·ªõn khi·∫øn motor ch·∫°y ch·∫≠m h·∫≥n d√≤ng ƒëi·ªán tƒÉng g·∫•p ƒë√¥i",
            "kh√¥ng ch√°y kh√¥ng kh√©t ch·ªâ n√≥ng rung do k√©o t·∫£i v∆∞·ª£t c√¥ng su·∫•t m√°y",
            "qu√° t·∫£i l√†m motor n√≥ng rung m·∫°nh ampe tƒÉng cao d√¢y ƒëai tr∆∞·ª£t",
            "thi·∫øt b·ªã k·∫πt do t·∫£i qu√° l·ªõn n√≥ng rung gi·∫≠t motor ch·∫°y ch·∫≠m",
        ],
        "severity_base": 0.8,
        "description": "Qu√° nhi·ªát + rung ƒë·ªông ‚Äî thi·∫øt b·ªã b·ªã qu√° t·∫£i",
    },
    "R√≤ r·ªâ h·ªá th·ªëng": {
        "samples": [
            "r√≤ r·ªâ d·∫ßu ch·∫£y d·∫ßu d·∫ßu loang",
            "r√≤ r·ªâ n∆∞·ªõc x√¨ h∆°i ch·∫£y n∆∞·ªõc",
            "gioƒÉng h·ªèng r√≤ r·ªâ r·ªâ d·∫ßu tr√†n d·∫ßu",
            "ph·ªõt h·ªèng r√≤ r·ªâ d·∫ßu √°p su·∫•t gi·∫£m",
            "seal h·ªèng r√≤ r·ªâ n∆∞·ªõc ch·∫£y tr√†n",
            "d·∫ßu ch·∫£y tr√†n ra n·ªÅn gioƒÉng b·ªã r√°ch h·ªèng n·∫∑ng",
            "r√≤ r·ªâ d·∫ßu th·ªßy l·ª±c t·ª´ ·ªëng n·ªëi d·∫ßu loang kh·∫Øp n∆°i",
            "ph·ªõt b∆°m h·ªèng g√¢y r√≤ r·ªâ n∆∞·ªõc li√™n t·ª•c",
            "x√¨ h∆°i t·ª´ van √°p su·∫•t gi·∫£m do r√≤ r·ªâ kh√≠",
            "seal b·ªã m√≤n g√¢y r·ªâ d·∫ßu t·ª´ tr·ª•c b∆°m",
            "r√≤ r·ªâ n∆∞·ªõc l√†m m√°t t·ª´ ƒë∆∞·ªùng ·ªëng ch·∫£y n∆∞·ªõc li√™n t·ª•c",
            "d·∫ßu r√≤ r·ªâ t·ª´ h·ªôp s·ªë v·∫øt d·∫ßu loang tr√™n s√†n",
            "b∆°m th·ªßy l·ª±c r√≤ d·∫ßu t·ª´ ph·ªõt tr·ª•c √°p su·∫•t s·ª•t",
            "r·ªâ d·∫ßu nh·ªè gi·ªçt li√™n t·ª•c t·ª´ ƒë√°y thi·∫øt b·ªã",
            "x√¨ kh√≠ t·ª´ ƒë∆∞·ªùng ·ªëng √°p su·∫•t cao r√≤ r·ªâ nghi√™m tr·ªçng",
            # --- New patterns (negation + technical) ---
            "kh√¥ng n√≥ng kh√¥ng rung nh∆∞ng ph√°t hi·ªán v≈©ng d·∫ßu d∆∞·ªõi ƒë√°y m√°y n√©n",
            "·ªëng d·∫´n d·∫ßu th·ªßy l·ª±c b·ªã r·∫°n n·ª©t g√¢y r√≤ r·ªâ nh·ªè gi·ªçt li√™n t·ª•c",
            "van x·∫£ an to√†n b·ªã x√¨ h∆°i li√™n t·ª•c √°p su·∫•t b√¨nh ch·ª©a gi·∫£m d·∫ßn",
            "m·∫∑t b√≠ch ƒë∆∞·ªùng ·ªëng b·ªã ch·∫£y n∆∞·ªõc t·∫°i v·ªã tr√≠ gioƒÉng l·∫Øp ƒë·∫∑t",
            "cylinder th·ªßy l·ª±c b·ªã r·ªâ d·∫ßu t·ª´ v·ªã tr√≠ ph·ªõt tr∆∞·ª£t h√†nh tr√¨nh kh√¥ng ƒë·ªß",
            "h·ªá th·ªëng kh√≠ n√©n b·ªã x√¨ h∆°i nhi·ªÅu v·ªã tr√≠ compressor ch·∫°y li√™n t·ª•c b√π √°p",
            "b·ªÉ ch·ª©a d·∫ßu gi·∫£m m·ª©c b√°o ƒë·ªông d√π kh√¥ng s·ª≠ d·ª•ng nghi r√≤ r·ªâ ƒë∆∞·ªùng ·ªëng",
            "kh√¥ng ·ªìn kh√¥ng n√≥ng ch·ªâ ph√°t hi·ªán v·∫øt d·∫ßu r√≤ r·ªâ d∆∞·ªõi m√°y",
            "ph·ªõt tr·ª•c ch√≠nh b·ªã h·ªèng g√¢y r√≤ d·∫ßu th·ªßy l·ª±c n·∫∑ng",
            "r√≤ r·ªâ kh√≠ n√©n t·ª´ kh·ªõp n·ªëi ·ªëng √°p su·∫•t gi·∫£m d·∫ßn li√™n t·ª•c",
        ],
        "severity_base": 0.5,
        "description": "R√≤ r·ªâ d·∫ßu, n∆∞·ªõc, ho·∫∑c kh√≠ trong h·ªá th·ªëng",
    },
    "H∆∞ h·ªèng c∆° kh√≠": {
        "samples": [
            "n·ª©t v·ª° g√£y bi·∫øn d·∫°ng cong v√™nh",
            "m√≤n nhi·ªÅu ƒÉn m√≤n g·ªâ s√©t han g·ªâ",
            "ƒë·ª©t d√¢y ƒëai d√¢y ƒëai m√≤n tu·ªôt",
            "l·ªèng bu l√¥ng lung lay tr·ª•c l·ªách tr·ª•c cong",
            "b·∫°c ƒë·∫°n h·ªèng v√≤ng bi h·ªèng m√≤n nhi·ªÅu",
            "tr·ª•c b·ªã cong v√™nh bu l√¥ng l·ªèng nhi·ªÅu ch·ªó",
            "d√¢y ƒëai b·ªã ƒë·ª©t r√°ch kh√¥ng truy·ªÅn ƒë·ªông ƒë∆∞·ª£c",
            "b√°nh rƒÉng b·ªã m√≤n nhi·ªÅu ƒÉn kh·ªõp kh√¥ng ƒë·ªÅu",
            "g·ªâ s√©t n·∫∑ng b·ªÅ m·∫∑t kim lo·∫°i han g·ªâ ƒÉn m√≤n",
            "th√¢n m√°y b·ªã n·ª©t v·ª° bi·∫øn d·∫°ng nghi√™m tr·ªçng",
            "bu l√¥ng c·ªë ƒë·ªãnh b·ªã l·ªèng thi·∫øt b·ªã lung lay",
            "tr·ª•c truy·ªÅn ƒë·ªông b·ªã cong l·ªách t√¢m nghi√™m tr·ªçng",
            "chi ti·∫øt m√°y b·ªã g√£y n·ª©t do m·ªèi v·∫≠t li·ªáu",
            "kh·ªõp n·ªëi b·ªã m√≤n x∆∞·ªõc ƒÉn m√≤n b·ªÅ m·∫∑t n·∫∑ng",
            "v·ªè thi·∫øt b·ªã b·ªã n·ª©t v·ª° do va ƒë·∫≠p c∆° kh√≠",
            # --- New patterns (negation + technical) ---
            "kh√¥ng ·ªìn kh√¥ng n√≥ng nh∆∞ng ki·ªÉm tra th·∫•y n·ª©t th√¢n v·ªè m√°y b∆°m",
            "b√°nh rƒÉng h·ªôp s·ªë b·ªã s·ª©t m·∫ª nhi·ªÅu rƒÉng ch·∫°y gi·∫≠t c·ª•c khi v√†o t·∫£i",
            "ch·ªët kh·ªõp n·ªëi gi·ªØa motor v√† b∆°m b·ªã c·∫Øt ƒë·ª©t do m·ªèi kim lo·∫°i",
            "c√°nh qu·∫°t b·ªã n·ª©t g√£y m·∫•t c√¢n b·∫±ng g√¢y rung khi ch·∫°y t·ªëc ƒë·ªô cao",
            "tr·ª•c truy·ªÅn ƒë·ªông b·ªã xo·∫Øn bi·∫øn d·∫°ng sau s·ª± c·ªë k·∫πt t·∫£i ƒë·ªôt ng·ªôt",
            "ƒë·∫ø l·∫Øp thi·∫øt b·ªã b·ªã n·ª©t foundation bu l√¥ng neo b·ªã nh·ªï g√¢y l·ªách tr·ª•c",
            "kh·ªõp n·ªëi m·ªÅm b·ªã r√°ch v·ª° cao su ƒë·ªám kh√¥ng truy·ªÅn l·ª±c hi·ªáu qu·∫£",
            "kh√¥ng n√≥ng kh√¥ng rung nh∆∞ng ph√°t hi·ªán n·ª©t v·ª° c∆° kh√≠ khi ki·ªÉm tra",
            "bu l√¥ng c·ªë ƒë·ªãnh l·ªèng thi·∫øt b·ªã lung lay khi ch·∫°y h∆∞ h·ªèng c∆° kh√≠",
            "ph√°t hi·ªán g√£y n·ª©t chi ti·∫øt m√°y do m·ªèi v·∫≠t li·ªáu sau ki·ªÉm tra",
        ],
        "severity_base": 0.6,
        "description": "H∆∞ h·ªèng c√°c b·ªô ph·∫≠n c∆° kh√≠",
    },
    "√Çm thanh b·∫•t th∆∞·ªùng": {
        "samples": [
            "ti·∫øng ·ªìn l·∫° ti·∫øng k√™u b·∫•t th∆∞·ªùng",
            "·ªìn l·ªõn ti·∫øng r√≠t ti·∫øng c·ªç s√°t",
            "ti·∫øng va ƒë·∫≠p ti·∫øng n·ªï g·∫ßm",
            "ti·∫øng k·∫πt ti·∫øng √π ti·∫øng l·∫°",
            "k√™u to k√™u l·ªõn ti·∫øng c·∫°ch c·∫°ch",
            "ti·∫øng ·ªìn b·∫•t th∆∞·ªùng k√™u to li√™n t·ª•c t·ª´ motor",
            "ti·∫øng r√≠t cao ph√°t ra t·ª´ thi·∫øt b·ªã khi v·∫≠n h√†nh",
            "ti·∫øng va ƒë·∫≠p l·ªõn b√™n trong m√°y ·ªìn b·∫•t th∆∞·ªùng",
            "ti·∫øng √π to li√™n t·ª•c t·ª´ motor ch√≠nh",
            "thi·∫øt b·ªã ph√°t ti·∫øng k·∫πt nghi·∫øn khi ho·∫°t ƒë·ªông",
            "√¢m thanh l·∫° ph√°t ra li√™n t·ª•c ·ªìn h∆°n b√¨nh th∆∞·ªùng",
            "ti·∫øng n·ªï nh·ªè l√°ch t√°ch li√™n t·ª•c t·ª´ thi·∫øt b·ªã",
            "ph√°t ra ti·∫øng c·ªç s√°t l·∫° khi motor quay",
            "ti·∫øng ·ªìn l·ªõn b·∫•t th∆∞·ªùng khi thi·∫øt b·ªã kh·ªüi ƒë·ªông",
            "ti·∫øng k√™u b·∫•t th∆∞·ªùng nghe r√µ t·ª´ xa m√°y ch·∫°y",
            # --- New patterns (negation + varied) ---
            "thi·∫øt b·ªã kh√¥ng n√≥ng kh√¥ng rung nh∆∞ng ph√°t ra ti·∫øng √π v√π li√™n t·ª•c l·∫°",
            "nghe ti·∫øng t√°ch t√°ch ƒë·ªÅu ƒë·∫∑n b√™n trong h·ªôp s·ªë khi motor ch·∫°y",
            "motor ph√°t ra ti·∫øng hu√Ωt s√°o cao t·∫ßn khi tƒÉng t·ªëc b·∫•t th∆∞·ªùng",
            "ti·∫øng g√µ l·ªõn nh·ªãp ƒë·ªÅu ph√°t ra t·ª´ ƒë·∫ßu piston m√°y n√©n m·ªói khi n√©n",
            "qu·∫°t h√∫t ph√°t ra ti·∫øng rung r·ªÅn l·∫° khi t·ªëc ƒë·ªô gi√≥ thay ƒë·ªïi",
            "ti·∫øng l·∫°ch x·∫°ch li√™n t·ª•c t·ª´ b√™n trong motor d√π kh√¥ng t·∫£i",
            "√¢m vang b·∫•t th∆∞·ªùng khi ch·∫°y kh√¥ng t·∫£i t·∫Øt m√°y th√¨ h·∫øt ti·∫øng",
            "kh√¥ng n√≥ng kh√¥ng kh√©t nh∆∞ng ti·∫øng ·ªìn b·∫•t th∆∞·ªùng ph√°t ra t·ª´ motor",
            "ti·∫øng r·ªÅn l·∫° ph√°t ra t·ª´ thi·∫øt b·ªã khi v·∫≠n h√†nh √¢m thanh b·∫•t th∆∞·ªùng",
            "thi·∫øt b·ªã ph√°t ti·∫øng ·ªìn l·∫° nghe r√µ khi ch·∫°y kh√¥ng t·∫£i",
        ],
        "severity_base": 0.5,
        "description": "Ph√°t hi·ªán √¢m thanh b·∫•t th∆∞·ªùng t·ª´ thi·∫øt b·ªã",
    },
    "Gi·∫£m hi·ªáu su·∫•t": {
        "samples": [
            "ch·∫°y ch·∫≠m y·∫øu c√¥ng su·∫•t gi·∫£m",
            "kh√¥ng kh·ªüi ƒë·ªông kh√¥ng ch·∫°y d·ª´ng ƒë·ªôt ng·ªôt",
            "t·∫Øt ƒë·ªôt ng·ªôt ch·∫≠p ch·ªùn kh√¥ng ·ªïn ƒë·ªãnh",
            "ho·∫°t ƒë·ªông ch·∫≠m k·∫πt treo m√°y",
            "qu√° t·ªëc hi·ªáu su·∫•t th·∫•p nƒÉng su·∫•t gi·∫£m",
            "m√°y ch·∫≠p ch·ªùn t·∫Øt b·∫≠t li√™n t·ª•c kh√¥ng ·ªïn ƒë·ªãnh",
            "thi·∫øt b·ªã ch·∫°y ch·∫≠m h·∫≥n c√¥ng su·∫•t s·ª•t gi·∫£m r√µ r·ªát",
            "motor kh√¥ng kh·ªüi ƒë·ªông ƒë∆∞·ª£c b·∫•m n√∫t kh√¥ng ph·∫£n h·ªìi",
            "m√°y d·ª´ng ƒë·ªôt ng·ªôt gi·ªØa ch·ª´ng khi ƒëang v·∫≠n h√†nh",
            "thi·∫øt b·ªã ho·∫°t ƒë·ªông y·∫øu kh√¥ng ƒë·∫°t c√¥ng su·∫•t thi·∫øt k·∫ø",
            "motor ch·∫°y ch·∫≠m h∆°n b√¨nh th∆∞·ªùng nƒÉng su·∫•t gi·∫£m",
            "thi·∫øt b·ªã t·∫Øt ƒë·ªôt ng·ªôt kh√¥ng kh·ªüi ƒë·ªông l·∫°i ƒë∆∞·ª£c",
            "hi·ªáu su·∫•t gi·∫£m r√µ r·ªát m√°y ch·∫°y y·∫øu h·∫≥n",
            "thi·∫øt b·ªã kh√¥ng ƒë·∫°t t·ªëc ƒë·ªô y√™u c·∫ßu ch·∫°y ch·∫≠m",
            "c√¥ng su·∫•t ƒë·∫ßu ra gi·∫£m m·∫°nh so v·ªõi th√¥ng s·ªë thi·∫øt k·∫ø",
            # --- New patterns (negation + operational) ---
            "kh√¥ng ·ªìn kh√¥ng n√≥ng nh∆∞ng m√°y ch·∫°y ng√†y c√†ng ch·∫≠m c√¥ng su·∫•t s·ª•t r√µ r·ªát",
            "b∆°m b∆°m kh√¥ng ƒë·ªß l∆∞u l∆∞·ª£ng d√π motor ch·∫°y ƒë·ªß v√≤ng tua nghi c√°nh b∆°m m√≤n",
            "motor kh·ªüi ƒë·ªông l√¢u h∆°n b√¨nh th∆∞·ªùng m·∫•t g·∫ßn 30 gi√¢y m·ªõi ƒë·∫°t v√≤ng quay",
            "thi·∫øt b·ªã t·ª± ng·∫Øt gi·ªØa ch·ª´ng r·ªìi kh·ªüi ƒë·ªông l·∫°i li√™n t·ª•c kh√¥ng ·ªïn ƒë·ªãnh",
            "nƒÉng su·∫•t s·∫£n xu·∫•t gi·∫£m 30 ph·∫ßn trƒÉm so v·ªõi th√°ng tr∆∞·ªõc d√π c√πng t·∫£i",
            "qu·∫°t quay ch·∫≠m h∆°n b√¨nh th∆∞·ªùng d√π ƒëi·ªán √°p cung c·∫•p ƒë√∫ng th√¥ng s·ªë",
            "motor ch·∫°y nh∆∞ng moment xo·∫Øn y·∫øu kh√¥ng ƒë·ªß k√©o t·∫£i nh∆∞ thi·∫øt k·∫ø",
            "kh√¥ng n√≥ng kh√¥ng rung nh∆∞ng c√¥ng su·∫•t ƒë·∫ßu ra gi·∫£m r√µ r·ªát hi·ªáu su·∫•t th·∫•p",
            "thi·∫øt b·ªã ch·∫°y y·∫øu h·∫≥n hi·ªáu su·∫•t gi·∫£m d·∫ßn theo th·ªùi gian",
            "m√°y ho·∫°t ƒë·ªông nh∆∞ng nƒÉng su·∫•t th·∫•p kh√¥ng ƒë·∫°t y√™u c·∫ßu s·∫£n xu·∫•t",
        ],
        "severity_base": 0.55,
        "description": "Thi·∫øt b·ªã ho·∫°t ƒë·ªông kh√¥ng ƒë·∫°t hi·ªáu su·∫•t mong ƒë·ª£i",
    },
}


# ============================================================
# 3. SYMPTOM KEYWORD DATABASE (for keyword extraction display)
# ============================================================

SYMPTOM_KEYWORDS = {
    "Nhi·ªát ƒë·ªô": [
        "n√≥ng b·∫•t th∆∞·ªùng", "n√≥ng ch·∫£y", "nhi·ªát ƒë·ªô cao", "nhi·ªát ƒë·ªô r·∫•t cao",
        "qu√° nhi·ªát", "qu√° n√≥ng", "n√≥ng", "n√≥ng ran", "t·ªèa nhi·ªát",
        "ch√°y tay", "b·ªèng tay", "kh√≥i", "b·ªëc kh√≥i", "h∆°i n√≥ng",
    ],
    "Rung ƒë·ªông": [
        "rung m·∫°nh", "rung b·∫•t th∆∞·ªùng", "rung l·∫Øc", "rung l·∫Øc m·∫°nh",
        "rung", "rung nh·∫π", "rung li√™n t·ª•c", "dao ƒë·ªông m·∫°nh",
        "gi·∫≠t", "gi·∫≠t c·ª•c", "lung lay", "x√≥c",
    ],
    "√Çm thanh": [
        "ti·∫øng kim lo·∫°i va ch·∫°m", "ti·∫øng kim lo·∫°i", "ti·∫øng k√™u l·∫°",
        "ti·∫øng k√™u b·∫•t th∆∞·ªùng", "·ªìn b·∫•t th∆∞·ªùng", "·ªìn l·ªõn", "ti·∫øng ·ªìn l·∫°",
        "ti·∫øng r√≠t", "ti·∫øng r√≠t cao", "ti·∫øng c·ªç s√°t", "ti·∫øng va ƒë·∫≠p",
        "ti·∫øng √π", "ti·∫øng l·∫°", "ti·∫øng k·∫πt", "ti·∫øng n·ªï", "n·ªï",
        "ti·∫øng c·∫°ch c·∫°ch", "ti·∫øng l√°ch c√°ch", "g·∫ßm",
    ],
    "M√πi": [
        "m√πi kh√©t", "m√πi ch√°y", "kh√©t", "m√πi d·∫ßu ch√°y", "m√πi d·∫ßu",
        "m√πi nh·ªõt ch√°y", "m√πi cao su ch√°y", "m√πi nh·ª±a ch√°y",
        "m√πi l·∫°", "m√πi h√¥i", "m√πi h·∫Øc", "b·ªëc m√πi", "ch√°y kh√©t",
    ],
    "ƒêi·ªán": [
        "d√≤ng ƒëi·ªán tƒÉng ƒë·ªôt ng·ªôt", "d√≤ng ƒëi·ªán tƒÉng", "d√≤ng ƒëi·ªán dao ƒë·ªông",
        "d√≤ng ƒëi·ªán b·∫•t th∆∞·ªùng", "ch·∫≠p ƒëi·ªán", "ƒë√°nh l·ª≠a", "ph√≥ng ƒëi·ªán",
        "tia l·ª≠a", "tia l·ª≠a ƒëi·ªán", "ƒëi·ªán gi·∫≠t", "r√≤ ƒëi·ªán", "ch·∫≠p m·∫°ch",
        "ch√°y c·∫ßu ch√¨", "qu√° t·∫£i", "s·ª•t √°p", "m·∫•t pha", "l·ªách pha",
    ],
    "R√≤ r·ªâ": [
        "r√≤ r·ªâ d·∫ßu", "r√≤ r·ªâ n∆∞·ªõc", "r√≤ r·ªâ", "ch·∫£y d·∫ßu", "ch·∫£y n∆∞·ªõc",
        "r·ªâ d·∫ßu", "d·∫ßu loang", "x√¨", "x√¨ h∆°i", "tr√†n d·∫ßu",
    ],
    "C∆° kh√≠": [
        "g√£y", "n·ª©t", "v·ª°", "m√≤n", "m√≤n nhi·ªÅu", "ƒÉn m√≤n", "g·ªâ s√©t",
        "han g·ªâ", "bi·∫øn d·∫°ng", "cong v√™nh", "l·ªèng", "l·ªèng bu l√¥ng",
        "tu·ªôt", "ƒë·ª©t d√¢y ƒëai", "d√¢y ƒëai m√≤n", "b·∫°c ƒë·∫°n h·ªèng",
        "b·∫°c ƒë·∫°n", "v√≤ng bi", "v√≤ng bi h·ªèng", "tr·ª•c b·ªã cong", "tr·ª•c l·ªách",
    ],
    "Hi·ªáu su·∫•t": [
        "ch·∫°y ch·∫≠m", "y·∫øu", "c√¥ng su·∫•t gi·∫£m", "kh√¥ng kh·ªüi ƒë·ªông",
        "kh√¥ng ch·∫°y", "d·ª´ng ƒë·ªôt ng·ªôt", "t·∫Øt ƒë·ªôt ng·ªôt", "ch·∫≠p ch·ªùn",
        "kh√¥ng ·ªïn ƒë·ªãnh", "ho·∫°t ƒë·ªông ch·∫≠m", "k·∫πt", "treo", "qu√° t·ªëc",
    ],
}

# Vietnamese negation words
NEGATION_WORDS = [
    "kh√¥ng c√≥", "kh√¥ng b·ªã", "kh√¥ng th·∫•y", "kh√¥ng nghe",
    "kh√¥ng ph√°t hi·ªán", "kh√¥ng c√≤n", "kh√¥ng h·ªÅ",
    "ch∆∞a c√≥", "ch∆∞a b·ªã", "ch∆∞a th·∫•y", "ch∆∞a ph√°t hi·ªán",
    "h·∫øt", "ƒë√£ h·∫øt", "kh√¥ng", "ch∆∞a",
]


# ============================================================
# 4. RECOMMENDATION DATABASE
# ============================================================

RECOMMENDATIONS_DB = {
    "Ch√°y cu·ªôn d√¢y / ch√°y motor": [
        "üö® D·ª™NG THI·∫æT B·ªä NGAY L·∫¨P T·ª®C",
        "Ng·∫Øt ngu·ªìn ƒëi·ªán v√† ƒë·∫£m b·∫£o an to√†n khu v·ª±c",
        "Ki·ªÉm tra c√°ch ƒëi·ªán cu·ªôn d√¢y (megger test)",
        "Ki·ªÉm tra h·ªá th·ªëng l√†m m√°t v√† qu·∫°t gi√≥",
        "ƒê√°nh gi√° l·∫°i ƒëi·ªÅu ki·ªán t·∫£i ‚Äî c√≥ th·ªÉ qu√° t·∫£i",
        "Li√™n h·ªá k·ªπ s∆∞ ƒëi·ªán ƒë·ªÉ ki·ªÉm tra chuy√™n s√¢u",
    ],
    "H·ªèng b·∫°c ƒë·∫°n / v√≤ng bi": [
        "üö® D·ª™NG THI·∫æT B·ªä ƒë·ªÉ tr√°nh h∆∞ h·ªèng th√™m",
        "Ki·ªÉm tra b·∫°c ƒë·∫°n / v√≤ng bi ‚Äî thay th·∫ø n·∫øu c·∫ßn",
        "Ki·ªÉm tra h·ªá th·ªëng b√¥i tr∆°n ‚Äî b·ªï sung m·ª° b√¥i tr∆°n",
        "Ki·ªÉm tra ƒë·ªô ƒë·ªìng tr·ª•c (alignment) c√°c kh·ªõp n·ªëi",
        "Ki·ªÉm tra c√¢n b·∫±ng ƒë·ªông rotor",
    ],
    "Qu√° t·∫£i c∆° kh√≠": [
        "üö® GI·∫¢M T·∫¢I NGAY ho·∫∑c d·ª´ng thi·∫øt b·ªã",
        "Ki·ªÉm tra ƒëi·ªÅu ki·ªán t·∫£i hi·ªán t·∫°i so v·ªõi th√¥ng s·ªë thi·∫øt k·∫ø",
        "Ki·ªÉm tra h·ªá th·ªëng truy·ªÅn ƒë·ªông (d√¢y ƒëai, kh·ªõp n·ªëi, h·ªôp s·ªë)",
        "Ki·ªÉm tra h·ªá th·ªëng l√†m m√°t",
        "ƒê√°nh gi√° l·∫°i quy tr√¨nh v·∫≠n h√†nh",
    ],
    "S·ª± c·ªë ƒëi·ªán": [
        "üö® NG·∫ÆT NGU·ªíN ƒêI·ªÜN NGAY",
        "Ki·ªÉm tra c√°ch ƒëi·ªán to√†n b·ªô h·ªá th·ªëng",
        "Ki·ªÉm tra ƒëi·ªán √°p, d√≤ng ƒëi·ªán, h·ªá s·ªë c√¥ng su·∫•t",
        "Ki·ªÉm tra t·ªß ƒëi·ªán, CB, contactor, relay b·∫£o v·ªá",
        "Ki·ªÉm tra ti·∫øp ƒë·ªãa v√† h·ªá th·ªëng b·∫£o v·ªá",
        "Li√™n h·ªá k·ªπ s∆∞ ƒëi·ªán chuy√™n tr√°ch",
    ],
    "Qu√° nhi·ªát": [
        "Gi·∫£m t·∫£i ho·∫∑c d·ª´ng thi·∫øt b·ªã ƒë·ªÉ h·∫° nhi·ªát",
        "Ki·ªÉm tra h·ªá th·ªëng l√†m m√°t (qu·∫°t, n∆∞·ªõc, d·∫ßu)",
        "Ki·ªÉm tra ƒëi·ªÅu ki·ªán m√¥i tr∆∞·ªùng (th√¥ng gi√≥, nhi·ªát ƒë·ªô xung quanh)",
        "Ki·ªÉm tra h·ªá th·ªëng b√¥i tr∆°n",
        "Theo d√µi nhi·ªát ƒë·ªô b·∫±ng camera nhi·ªát n·∫øu c√≥",
    ],
    "R√≤ r·ªâ h·ªá th·ªëng": [
        "X√°c ƒë·ªãnh v·ªã tr√≠ r√≤ r·ªâ ch√≠nh x√°c",
        "Ki·ªÉm tra gioƒÉng, ph·ªõt, seal ‚Äî thay th·∫ø n·∫øu h·ªèng",
        "Ki·ªÉm tra √°p su·∫•t h·ªá th·ªëng",
        "B·ªï sung d·∫ßu/n∆∞·ªõc n·∫øu thi·∫øu",
        "L√™n k·∫ø ho·∫°ch b·∫£o tr√¨ thay th·∫ø seal",
    ],
    "H∆∞ h·ªèng c∆° kh√≠": [
        "Ki·ªÉm tra chi ti·∫øt b·ªô ph·∫≠n b·ªã h∆∞ h·ªèng",
        "ƒê√°nh gi√° m·ª©c ƒë·ªô h∆∞ h·ªèng ‚Äî s·ª≠a ch·ªØa ho·∫∑c thay th·∫ø",
        "Ki·ªÉm tra c√°c b·ªô ph·∫≠n li√™n quan c√≥ b·ªã ·∫£nh h∆∞·ªüng kh√¥ng",
        "L√™n k·∫ø ho·∫°ch thay th·∫ø ph·ª• t√πng",
        "R√† so√°t l·∫°i quy tr√¨nh b·∫£o tr√¨ ƒë·ªãnh k·ª≥",
    ],
    "√Çm thanh b·∫•t th∆∞·ªùng": [
        "X√°c ƒë·ªãnh v·ªã tr√≠ v√† ƒë·∫∑c ƒëi·ªÉm √¢m thanh",
        "Ki·ªÉm tra c√°c b·ªô ph·∫≠n quay: b·∫°c ƒë·∫°n, tr·ª•c, b√°nh rƒÉng",
        "Ki·ªÉm tra h·ªá th·ªëng truy·ªÅn ƒë·ªông (d√¢y ƒëai, x√≠ch)",
        "Ki·ªÉm tra l·ªèng k·∫øt n·ªëi c∆° kh√≠",
        "S·ª≠ d·ª•ng stethoscope c√¥ng nghi·ªáp ƒë·ªÉ ch·∫©n ƒëo√°n ch√≠nh x√°c",
    ],
    "Gi·∫£m hi·ªáu su·∫•t": [
        "Ki·ªÉm tra ƒëi·ªÅu ki·ªán ƒë·∫ßu v√†o (ƒëi·ªán, kh√≠, n∆∞·ªõc)",
        "Ki·ªÉm tra b·ªô l·ªçc ‚Äî v·ªá sinh ho·∫∑c thay th·∫ø",
        "Ki·ªÉm tra h·ªá th·ªëng truy·ªÅn ƒë·ªông ‚Äî d√¢y ƒëai, ly h·ª£p",
        "ƒê√°nh gi√° l·∫°i th√¥ng s·ªë v·∫≠n h√†nh",
        "L√™n k·∫ø ho·∫°ch b·∫£o tr√¨ t·ªïng th·ªÉ",
    ],
    "_default": [
        "Ti·∫øp t·ª•c theo d√µi thi·∫øt b·ªã",
        "B·∫£o d∆∞·ª°ng ƒë·ªãnh k·ª≥ theo k·∫ø ho·∫°ch",
        "Ghi nh·∫≠n t√¨nh tr·∫°ng ƒë·ªÉ theo d√µi xu h∆∞·ªõng",
    ],
}


# AnalysisResult imported from base_engine.py


# ============================================================
# 6. PhoBERT ENGINE CLASS
# ============================================================

class PhoBERTEngine(BaseNLPEngine):
    """
    PhoBERT Engine cho ph√¢n t√≠ch thi·∫øt b·ªã c√¥ng nghi·ªáp.

    Pipeline:
    1. Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n (normalize, clean)
    2. PhoBERT Tokenization & Encoding
    3. Cosine Similarity v·ªõi c√°c m·∫´u l·ªói tham chi·∫øu
    4. Ph√¢n lo·∫°i l·ªói (semantic classification)
    5. Tr√≠ch xu·∫•t keyword (supplementary)
    6. ƒê√°nh gi√° severity
    7. Sinh khuy·∫øn ngh·ªã
    """

    @property
    def name(self) -> str:
        return "phobert"

    def __init__(self):
        self.tokenizer = _tokenizer
        self.model = _model
        self.device = _device
        self.fault_refs = FAULT_REFERENCES
        self.recommendations_db = RECOMMENDATIONS_DB

        # Fine-tuned classifier (n·∫øu c√≥)
        self.finetuned_model = _finetuned_model
        self.finetuned_labels = _finetuned_labels
        self.use_finetuned = _finetuned_model is not None

        if self.use_finetuned:
            print("üî• PhoBERTEngine: using FINE-TUNED classifier")
        else:
            print("üîÑ PhoBERTEngine: using zero-shot similarity (fallback)")

        # Pre-compute embeddings (lu√¥n c·∫ßn cho severity assessment)
        self.ref_embeddings = {}
        self._precompute_reference_embeddings()

    def _precompute_reference_embeddings(self):
        """T√≠nh tr∆∞·ªõc embeddings cho t·∫•t c·∫£ m·∫´u tham chi·∫øu."""
        print("üîÑ Pre-computing reference embeddings...")
        for fault_name, fault_data in self.fault_refs.items():
            embeddings = []
            for sample in fault_data["samples"]:
                emb = self._encode_text(sample)
                embeddings.append(emb)
            # L·∫•y trung b√¨nh c√°c embeddings l√†m ƒë·∫°i di·ªán cho lo·∫°i l·ªói
            self.ref_embeddings[fault_name] = torch.stack(embeddings).mean(dim=0)
        print("‚úÖ Reference embeddings ready")

    # ----------------------------------------------------------
    # PhoBERT Encoding
    # ----------------------------------------------------------
    @torch.no_grad()
    def _encode_text(self, text: str) -> torch.Tensor:
        """
        Encode text th√†nh embedding vector s·ª≠ d·ª•ng PhoBERT.
        Tr·∫£ v·ªÅ [CLS] token embedding (768-dim).
        """
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=256,
        ).to(self.device)

        outputs = self.model(**inputs)
        # L·∫•y [CLS] token (v·ªã tr√≠ 0) t·ª´ last hidden state
        cls_embedding = outputs.last_hidden_state[:, 0, :]
        return cls_embedding.squeeze(0).cpu()

    # ----------------------------------------------------------
    # Step 1: Text Preprocessing
    # ----------------------------------------------------------
    def preprocess(self, text: str) -> str:
        """Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát."""
        text = unicodedata.normalize("NFC", text)
        text = text.lower()
        text = re.sub(r'[^\w\s√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    # ----------------------------------------------------------
    # Step 2: Keyword Extraction (supplementary)
    # ----------------------------------------------------------
    def extract_keywords(self, text: str) -> list:
        """
        Tr√≠ch xu·∫•t t·ª´ kh√≥a tri·ªáu ch·ª©ng t·ª´ text.
        B·ªï sung cho PhoBERT ‚Äî gi√∫p hi·ªÉn th·ªã keywords cho user.
        """
        found = []
        negated_spans = []

        # Collect all keywords, sort by length desc
        all_kw = []
        for category, keywords in SYMPTOM_KEYWORDS.items():
            for kw in keywords:
                all_kw.append((kw, category))
        all_kw.sort(key=lambda x: len(x[0]), reverse=True)

        matched_spans = []

        for kw, category in all_kw:
            pattern = re.compile(re.escape(kw), re.IGNORECASE)
            for match in pattern.finditer(text):
                start, end = match.span()

                # Check if in negated span
                in_negated = any(start >= ns and end <= ne for ns, ne in negated_spans)
                if in_negated:
                    continue

                # Check negation
                lookback = max(0, start - 25)
                preceding = text[lookback:start].strip().lower()
                is_neg = any(preceding.endswith(neg) for neg in NEGATION_WORDS)
                if is_neg:
                    negated_spans.append((start, end))
                    continue

                # Check overlap
                is_overlap = any(start < me and end > ms for ms, me in matched_spans)
                if not is_overlap:
                    matched_spans.append((start, end))
                    found.append({"keyword": kw, "category": category})

        return found

    # ----------------------------------------------------------
    # Step 3: Semantic Fault Classification (PhoBERT)
    # ----------------------------------------------------------
    def classify_fault_phobert(self, text: str) -> list:
        """
        Ph√¢n lo·∫°i l·ªói b·∫±ng PhoBERT.
        - N·∫øu c√≥ fine-tuned model ‚Üí softmax classifier
        - N·∫øu kh√¥ng ‚Üí fallback cosine similarity
        Returns: danh s√°ch (fault_name, score) ƒë√£ s·∫Øp x·∫øp gi·∫£m d·∫ßn.
        """
        if self.use_finetuned:
            return self._classify_finetuned(text)
        else:
            return self._classify_zero_shot(text)

    @torch.no_grad()
    def _classify_finetuned(self, text: str) -> list:
        """Ph√¢n lo·∫°i b·∫±ng fine-tuned classifier (softmax)."""
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=256,
        ).to(self.device)

        logits = self.finetuned_model(**inputs)
        probs = torch.softmax(logits, dim=1).squeeze(0).cpu()

        scores = []
        for idx, label in enumerate(self.finetuned_labels):
            scores.append((label, probs[idx].item()))

        scores.sort(key=lambda x: x[1], reverse=True)
        return scores

    def _classify_zero_shot(self, text: str) -> list:
        """Fallback: ph√¢n lo·∫°i b·∫±ng cosine similarity."""
        text_embedding = self._encode_text(text)

        scores = []
        for fault_name, ref_emb in self.ref_embeddings.items():
            similarity = torch.nn.functional.cosine_similarity(
                text_embedding.unsqueeze(0),
                ref_emb.unsqueeze(0),
            ).item()
            scores.append((fault_name, similarity))

        scores.sort(key=lambda x: x[1], reverse=True)
        return scores

    # ----------------------------------------------------------
    # Step 4: Severity Assessment
    # ----------------------------------------------------------
    def assess_severity(self, fault_type: str, similarity: float, keywords: list) -> tuple:
        """
        ƒê√°nh gi√° m·ª©c ƒë·ªô nghi√™m tr·ªçng.
        K·∫øt h·ª£p: PhoBERT similarity + severity_base c·ªßa lo·∫°i l·ªói + s·ªë keyword.
        """
        fault_data = self.fault_refs.get(fault_type, {})
        severity_base = fault_data.get("severity_base", 0.3)

        # K·∫øt h·ª£p severity: base * similarity + keyword bonus
        keyword_bonus = min(len(keywords) * 0.05, 0.2)
        severity_score = min(severity_base * similarity + keyword_bonus, 1.0)
        severity_score = round(severity_score, 2)

        if severity_score >= 0.65:
            return ("NGHI√äM TR·ªåNG", severity_score)
        elif severity_score >= 0.40:
            return ("C·∫¢NH B√ÅO", severity_score)
        else:
            return ("TH·∫§P", severity_score)

    # ----------------------------------------------------------
    # Step 5: Summary Generation
    # ----------------------------------------------------------
    def generate_summary(self, equipment: str, fault_type: str, severity: str, keywords: list, similarity: float) -> str:
        """T·∫°o t√≥m t·∫Øt."""
        if fault_type == "Ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh":
            return f"{equipment} ‚Äî kh√¥ng ph√°t hi·ªán tri·ªáu ch·ª©ng b·∫•t th∆∞·ªùng. Thi·∫øt b·ªã c√≥ th·ªÉ ƒëang ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng."

        fault_desc = self.fault_refs.get(fault_type, {}).get("description", fault_type)
        kw_list = ", ".join([k["keyword"] for k in keywords]) if keywords else "kh√¥ng r√µ"

        summary = f"{equipment} ‚Äî PhoBERT ph√¢n lo·∫°i: {fault_type} (similarity: {similarity:.1%}). "
        summary += f"T·ª´ kh√≥a ph√°t hi·ªán: {kw_list}. "
        summary += f"M√¥ t·∫£: {fault_desc}. M·ª©c ƒë·ªô: {severity}."
        return summary

    # ----------------------------------------------------------
    # MAIN PIPELINE
    # ----------------------------------------------------------
    def analyze(self, equipment: str, description: str) -> AnalysisResult:
        """
        Main NLP pipeline s·ª≠ d·ª•ng PhoBERT.

        Args:
            equipment: Lo·∫°i thi·∫øt b·ªã
            description: M√¥ t·∫£ t·ª± nhi√™n ti·∫øng Vi·ªát

        Returns:
            AnalysisResult
        """
        t0 = time.perf_counter()
        pipeline_steps = []

        # Step 1: Preprocessing
        cleaned = self.preprocess(description)
        pipeline_steps.append({
            "step": 1,
            "name": "Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n",
            "input": description,
            "output": cleaned,
        })

        # Step 2: PhoBERT Tokenization
        tokens = self.tokenizer.tokenize(cleaned)
        pipeline_steps.append({
            "step": 2,
            "name": "PhoBERT Tokenization",
            "input": cleaned,
            "output": tokens,
        })

        # Step 3: Keyword Extraction
        keywords = self.extract_keywords(cleaned)
        pipeline_steps.append({
            "step": 3,
            "name": "Tr√≠ch xu·∫•t t·ª´ kh√≥a",
            "input": cleaned,
            "output": keywords,
        })

        # Step 4: PhoBERT Classification
        classify_method = "Fine-tuned Classifier" if self.use_finetuned else "Cosine Similarity"
        scores = self.classify_fault_phobert(cleaned)
        top_5 = scores[:5]

        pipeline_steps.append({
            "step": 4,
            "name": f"PhoBERT Ph√¢n lo·∫°i l·ªói ({classify_method})",
            "input": "PhoBERT embedding (768-dim)",
            "output": [{
                "fault": ("‚úÖ " if self.fault_refs.get(f, {}).get("is_normal", False) else "‚ö†Ô∏è ") + f,
                "score": round(s, 4),
            } for f, s in top_5],
        })

        # --- DECISION LOGIC ---
        if self.use_finetuned:
            # Fine-tuned classifier: scores ƒë√£ l√† softmax probabilities, d√πng tr·ª±c ti·∫øp
            top_fault, top_score = scores[0]
            top_sim = top_score  # Probability thay cho similarity

            # Map label name n·∫øu c·∫ßn
            is_normal = (top_fault == "Ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh")

            if is_normal:
                fault_type = "Ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh"
                severity = "B√åNH TH∆Ø·ªúNG"
                severity_score = 0.0
                confidence = round(top_sim, 2)
                recommendations = RECOMMENDATIONS_DB["_default"]
            else:
                fault_type = top_fault
                severity, severity_score = self.assess_severity(fault_type, top_sim, keywords)
                confidence = round(top_sim, 2)
                recommendations = self.recommendations_db.get(fault_type, self.recommendations_db["_default"])
        else:
            # Zero-shot mode: √°p d·ª•ng keyword re-ranking + B√¨nh th∆∞·ªùng heuristics
            keyword_categories = set(k["category"] for k in keywords)

            CATEGORY_FAULT_MAP = {
                "Nhi·ªát ƒë·ªô": ["Qu√° nhi·ªát", "Ch√°y cu·ªôn d√¢y / ch√°y motor", "Qu√° t·∫£i c∆° kh√≠"],
                "Rung ƒë·ªông": ["H·ªèng b·∫°c ƒë·∫°n / v√≤ng bi", "Qu√° t·∫£i c∆° kh√≠"],
                "√Çm thanh": ["H·ªèng b·∫°c ƒë·∫°n / v√≤ng bi", "√Çm thanh b·∫•t th∆∞·ªùng"],
                "M√πi": ["Ch√°y cu·ªôn d√¢y / ch√°y motor"],
                "ƒêi·ªán": ["S·ª± c·ªë ƒëi·ªán", "Ch√°y cu·ªôn d√¢y / ch√°y motor"],
                "R√≤ r·ªâ": ["R√≤ r·ªâ h·ªá th·ªëng"],
                "C∆° kh√≠": ["H∆∞ h·ªèng c∆° kh√≠", "H·ªèng b·∫°c ƒë·∫°n / v√≤ng bi"],
                "Hi·ªáu su·∫•t": ["Gi·∫£m hi·ªáu su·∫•t", "Qu√° t·∫£i c∆° kh√≠"],
            }

            boosted_scores = []
            for fault_name, sim in scores:
                boost = 0.0
                for cat in keyword_categories:
                    related = CATEGORY_FAULT_MAP.get(cat, [])
                    if fault_name in related:
                        boost += 0.1
                boosted_scores.append((fault_name, sim + boost))

            boosted_scores.sort(key=lambda x: x[1], reverse=True)
            top_fault, top_score = boosted_scores[0]
            top_sim = dict(scores)[top_fault]

            is_normal = self.fault_refs.get(top_fault, {}).get("is_normal", False)

            if is_normal and len(keywords) > 0:
                for fname, fscore in boosted_scores:
                    if not self.fault_refs.get(fname, {}).get("is_normal", False):
                        top_fault = fname
                        top_score = fscore
                        top_sim = dict(scores)[fname]
                        break
                is_normal = False

            if is_normal and len(keywords) == 0:
                fault_type = "Ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh"
                severity = "B√åNH TH∆Ø·ªúNG"
                severity_score = 0.0
                confidence = round(top_sim, 2)
                recommendations = RECOMMENDATIONS_DB["_default"]
            else:
                fault_type = top_fault
                severity, severity_score = self.assess_severity(fault_type, top_sim, keywords)
                confidence = round(top_sim, 2)
                recommendations = self.recommendations_db.get(fault_type, self.recommendations_db["_default"])

        pipeline_steps.append({
            "step": 5,
            "name": "ƒê√°nh gi√° m·ª©c ƒë·ªô nghi√™m tr·ªçng",
            "input": f"fault={fault_type}, similarity={top_sim:.4f}, keywords={len(keywords)}",
            "output": {"severity": severity, "score": severity_score},
        })

        # Step 6: Recommendations
        pipeline_steps.append({
            "step": 6,
            "name": "Sinh khuy·∫øn ngh·ªã",
            "input": fault_type,
            "output": recommendations,
        })

        # Summary
        summary = self.generate_summary(equipment, fault_type, severity, keywords, top_sim)

        elapsed_ms = (time.perf_counter() - t0) * 1000

        return AnalysisResult(
            fault_type=fault_type,
            severity=severity,
            severity_score=severity_score,
            confidence=confidence,
            keywords=[k["keyword"] for k in keywords],
            symptoms=[{
                "keyword": k["keyword"],
                "category": k["category"],
                "label": k["keyword"],
                "weight": 3,
            } for k in keywords],
            recommendations=recommendations,
            summary=summary,
            pipeline_steps=pipeline_steps,
            engine_name=self.name,
            engine_latency_ms=round(elapsed_ms, 2),
        )


# ============================================================
# SINGLETON & CONVENIENCE
# ============================================================

engine = PhoBERTEngine()


def analyze(equipment: str, description: str) -> AnalysisResult:
    """Convenience function ‚Äî g·ªçi engine.analyze()."""
    return engine.analyze(equipment, description)
