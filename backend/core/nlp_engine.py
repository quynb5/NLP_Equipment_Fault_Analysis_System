"""
NLP Engine - Vietnamese Industrial Equipment Fault Analysis (PhoBERT)
=====================================================================
Pipeline: Vietnamese text â†’ Preprocessing â†’ PhoBERT Tokenization
           â†’ PhoBERT Encoding â†’ Semantic Fault Classification
           â†’ Severity Scoring â†’ Recommendation Generation

Sá»­ dá»¥ng PhoBERT (vinai/phobert-base) Ä‘á»ƒ encode mÃ´ táº£ thiáº¿t bá»‹,
sau Ä‘Ã³ so sÃ¡nh cosine similarity vá»›i cÃ¡c máº«u lá»—i Ä‘Ã£ Ä‘á»‹nh nghÄ©a
Ä‘á»ƒ phÃ¢n loáº¡i lá»—i vÃ  Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ nghiÃªm trá»ng.
"""

import re
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""  # Force CPU mode

import unicodedata
import torch
from dataclasses import dataclass, field
from transformers import AutoModel, AutoTokenizer


# ============================================================
# 1. PhoBERT MODEL LOADER
# ============================================================

print("ğŸ”„ Äang táº£i PhoBERT model (vinai/phobert-base)...")

import pathlib as _pathlib
_MODEL_PATH = str(_pathlib.Path(__file__).resolve().parent / "resources" / "phobert-base")

_tokenizer = AutoTokenizer.from_pretrained(_MODEL_PATH)
_model = AutoModel.from_pretrained(_MODEL_PATH)
_model.eval()  # Cháº¿ Ä‘á»™ inference

# Chá»n device
_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
_model.to(_device)

print(f"âœ… PhoBERT loaded on {_device}")


# ============================================================
# 2. FAULT REFERENCE DATABASE
# ============================================================

# CÃ¡c máº«u mÃ´ táº£ lá»—i tham chiáº¿u â€” PhoBERT sáº½ so sÃ¡nh semantic similarity
FAULT_REFERENCES = {
    "BÃ¬nh thÆ°á»ng": {
        "samples": [
            "thiáº¿t bá»‹ hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng á»•n Ä‘á»‹nh",
            "mÃ¡y cháº¡y tá»‘t khÃ´ng cÃ³ váº¥n Ä‘á» gÃ¬",
            "hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng khÃ´ng cÃ³ tiáº¿ng á»“n láº¡",
            "má»i thá»© á»•n Ä‘á»‹nh khÃ´ng phÃ¡t hiá»‡n báº¥t thÆ°á»ng",
            "thiáº¿t bá»‹ váº­n hÃ nh tá»‘t khÃ´ng rung khÃ´ng nÃ³ng",
            "mÃ¡y hoáº¡t Ä‘á»™ng Ãªm khÃ´ng cÃ³ mÃ¹i láº¡",
            "tÃ¬nh tráº¡ng tá»‘t nhiá»‡t Ä‘á»™ bÃ¬nh thÆ°á»ng",
            "motor cháº¡y Ãªm Ã¡i khÃ´ng cÃ³ tiáº¿ng Ä‘á»™ng láº¡",
            "hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng cháº¡y Ãªm Ã¡i",
            "mÃ¡y cháº¡y Ãªm khÃ´ng rung khÃ´ng nÃ³ng khÃ´ng mÃ¹i",
            "thiáº¿t bá»‹ cháº¡y á»•n Ä‘á»‹nh khÃ´ng cÃ³ báº¥t thÆ°á»ng gÃ¬",
            "hoáº¡t Ä‘á»™ng tá»‘t khÃ´ng cÃ³ sá»± cá»‘",
            "váº­n hÃ nh bÃ¬nh thÆ°á»ng khÃ´ng phÃ¡t hiá»‡n hÆ° há»ng",
            "mÃ¡y hoáº¡t Ä‘á»™ng tá»‘t khÃ´ng cáº§n báº£o trÃ¬",
            "táº¥t cáº£ chá»‰ sá»‘ bÃ¬nh thÆ°á»ng thiáº¿t bá»‹ á»•n Ä‘á»‹nh",
        ],
        "severity_base": 0.0,
        "is_normal": True,
        "description": "Thiáº¿t bá»‹ hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng, khÃ´ng phÃ¡t hiá»‡n báº¥t thÆ°á»ng",
    },
    "QuÃ¡ nhiá»‡t": {
        "samples": [
            "thiáº¿t bá»‹ nÃ³ng báº¥t thÆ°á»ng nhiá»‡t Ä‘á»™ ráº¥t cao",
            "motor quÃ¡ nÃ³ng tá»a nhiá»‡t máº¡nh chÃ¡y tay",
            "nhiá»‡t Ä‘á»™ tÄƒng cao báº¥t thÆ°á»ng bá»ng tay nÃ³ng ran",
            "thiáº¿t bá»‹ bá»‘c hÆ¡i nÃ³ng khÃ³i nhiá»‡t tÄƒng",
            "vá» mÃ¡y nÃ³ng cháº£y quÃ¡ nhiá»‡t nghiÃªm trá»ng",
        ],
        "severity_base": 0.7,
        "description": "Thiáº¿t bá»‹ hoáº¡t Ä‘á»™ng á»Ÿ nhiá»‡t Ä‘á»™ cao báº¥t thÆ°á»ng",
    },
    "Há»ng báº¡c Ä‘áº¡n / vÃ²ng bi": {
        "samples": [
            "rung máº¡nh kÃ¨m tiáº¿ng kim loáº¡i va cháº¡m",
            "tiáº¿ng kÃªu láº¡ rung láº¯c máº¡nh tiáº¿ng cá» sÃ¡t",
            "rung báº¥t thÆ°á»ng tiáº¿ng rÃ­t cao tiáº¿ng kim loáº¡i",
            "rung Ä‘á»™ng máº¡nh kÃ¨m tiáº¿ng va Ä‘áº­p báº¡c Ä‘áº¡n vÃ²ng bi",
            "tiáº¿ng lÃ¡ch cÃ¡ch rung liÃªn tá»¥c giáº­t cá»¥c",
        ],
        "severity_base": 0.75,
        "description": "Rung Ä‘á»™ng + tiáº¿ng kim loáº¡i â€” nghi ngá» há»ng báº¡c Ä‘áº¡n hoáº·c vÃ²ng bi",
    },
    "ChÃ¡y cuá»™n dÃ¢y / chÃ¡y motor": {
        "samples": [
            "mÃ¹i khÃ©t chÃ¡y kÃ¨m nhiá»‡t Ä‘á»™ ráº¥t cao bá»‘c khÃ³i",
            "mÃ¹i chÃ¡y mÃ¹i nhá»±a chÃ¡y nÃ³ng báº¥t thÆ°á»ng bá»‘c khÃ³i",
            "khÃ©t mÃ¹i dáº§u chÃ¡y quÃ¡ nhiá»‡t nghiÃªm trá»ng khÃ³i",
            "motor chÃ¡y mÃ¹i khÃ©t nÃ³ng cháº£y tia lá»­a Ä‘iá»‡n",
            "cuá»™n dÃ¢y chÃ¡y mÃ¹i cao su chÃ¡y bá»‘c khÃ³i nhiá»‡t cao",
        ],
        "severity_base": 0.9,
        "description": "QuÃ¡ nhiá»‡t káº¿t há»£p mÃ¹i chÃ¡y â€” chÃ¡y cuá»™n dÃ¢y hoáº·c chÃ¡y motor",
    },
    "Sá»± cá»‘ Ä‘iá»‡n": {
        "samples": [
            "dÃ²ng Ä‘iá»‡n tÄƒng Ä‘á»™t ngá»™t cháº­p máº¡ch phÃ³ng Ä‘iá»‡n",
            "tia lá»­a Ä‘iá»‡n rÃ² Ä‘iá»‡n cháº­p Ä‘iá»‡n",
            "dÃ²ng Ä‘iá»‡n dao Ä‘á»™ng báº¥t thÆ°á»ng quÃ¡ táº£i chÃ¡y cáº§u chÃ¬",
            "máº¥t pha lá»‡ch pha sá»¥t Ã¡p dÃ²ng Ä‘iá»‡n báº¥t thÆ°á»ng",
            "Ä‘iá»‡n giáº­t rÃ² Ä‘iá»‡n nguy hiá»ƒm cháº­p máº¡ch",
        ],
        "severity_base": 0.85,
        "description": "Váº¥n Ä‘á» há»‡ thá»‘ng Ä‘iá»‡n â€” cháº­p, rÃ², quÃ¡ táº£i",
    },
    "QuÃ¡ táº£i cÆ¡ khÃ­": {
        "samples": [
            "nÃ³ng báº¥t thÆ°á»ng kÃ¨m rung máº¡nh quÃ¡ táº£i",
            "nhiá»‡t Ä‘á»™ cao rung Ä‘á»™ng máº¡nh thiáº¿t bá»‹ quÃ¡ táº£i",
            "quÃ¡ nÃ³ng rung láº¯c máº¡nh dÃ¢y Ä‘ai cÄƒng quÃ¡ táº£i",
            "motor nÃ³ng rung máº¡nh cháº¡y cháº­m cÃ´ng suáº¥t giáº£m",
            "thiáº¿t bá»‹ quÃ¡ táº£i nÃ³ng rung giáº­t káº¹t",
        ],
        "severity_base": 0.8,
        "description": "QuÃ¡ nhiá»‡t + rung Ä‘á»™ng â€” thiáº¿t bá»‹ bá»‹ quÃ¡ táº£i",
    },
    "RÃ² rá»‰ há»‡ thá»‘ng": {
        "samples": [
            "rÃ² rá»‰ dáº§u cháº£y dáº§u dáº§u loang",
            "rÃ² rá»‰ nÆ°á»›c xÃ¬ hÆ¡i cháº£y nÆ°á»›c",
            "gioÄƒng há»ng rÃ² rá»‰ rá»‰ dáº§u trÃ n dáº§u",
            "phá»›t há»ng rÃ² rá»‰ dáº§u Ã¡p suáº¥t giáº£m",
            "seal há»ng rÃ² rá»‰ nÆ°á»›c cháº£y trÃ n",
        ],
        "severity_base": 0.5,
        "description": "RÃ² rá»‰ dáº§u, nÆ°á»›c, hoáº·c khÃ­ trong há»‡ thá»‘ng",
    },
    "HÆ° há»ng cÆ¡ khÃ­": {
        "samples": [
            "ná»©t vá»¡ gÃ£y biáº¿n dáº¡ng cong vÃªnh",
            "mÃ²n nhiá»u Äƒn mÃ²n gá»‰ sÃ©t han gá»‰",
            "Ä‘á»©t dÃ¢y Ä‘ai dÃ¢y Ä‘ai mÃ²n tuá»™t",
            "lá»ng bu lÃ´ng lung lay trá»¥c lá»‡ch trá»¥c cong",
            "báº¡c Ä‘áº¡n há»ng vÃ²ng bi há»ng mÃ²n nhiá»u",
        ],
        "severity_base": 0.6,
        "description": "HÆ° há»ng cÃ¡c bá»™ pháº­n cÆ¡ khÃ­",
    },
    "Ã‚m thanh báº¥t thÆ°á»ng": {
        "samples": [
            "tiáº¿ng á»“n láº¡ tiáº¿ng kÃªu báº¥t thÆ°á»ng",
            "á»“n lá»›n tiáº¿ng rÃ­t tiáº¿ng cá» sÃ¡t",
            "tiáº¿ng va Ä‘áº­p tiáº¿ng ná»• gáº§m",
            "tiáº¿ng káº¹t tiáº¿ng Ã¹ tiáº¿ng láº¡",
            "kÃªu to kÃªu lá»›n tiáº¿ng cáº¡ch cáº¡ch",
        ],
        "severity_base": 0.5,
        "description": "PhÃ¡t hiá»‡n Ã¢m thanh báº¥t thÆ°á»ng tá»« thiáº¿t bá»‹",
    },
    "Giáº£m hiá»‡u suáº¥t": {
        "samples": [
            "cháº¡y cháº­m yáº¿u cÃ´ng suáº¥t giáº£m",
            "khÃ´ng khá»Ÿi Ä‘á»™ng khÃ´ng cháº¡y dá»«ng Ä‘á»™t ngá»™t",
            "táº¯t Ä‘á»™t ngá»™t cháº­p chá»n khÃ´ng á»•n Ä‘á»‹nh",
            "hoáº¡t Ä‘á»™ng cháº­m káº¹t treo mÃ¡y",
            "quÃ¡ tá»‘c hiá»‡u suáº¥t tháº¥p nÄƒng suáº¥t giáº£m",
        ],
        "severity_base": 0.55,
        "description": "Thiáº¿t bá»‹ hoáº¡t Ä‘á»™ng khÃ´ng Ä‘áº¡t hiá»‡u suáº¥t mong Ä‘á»£i",
    },
}


# ============================================================
# 3. SYMPTOM KEYWORD DATABASE (for keyword extraction display)
# ============================================================

SYMPTOM_KEYWORDS = {
    "Nhiá»‡t Ä‘á»™": [
        "nÃ³ng báº¥t thÆ°á»ng", "nÃ³ng cháº£y", "nhiá»‡t Ä‘á»™ cao", "nhiá»‡t Ä‘á»™ ráº¥t cao",
        "quÃ¡ nhiá»‡t", "quÃ¡ nÃ³ng", "nÃ³ng", "nÃ³ng ran", "tá»a nhiá»‡t",
        "chÃ¡y tay", "bá»ng tay", "khÃ³i", "bá»‘c khÃ³i", "hÆ¡i nÃ³ng",
    ],
    "Rung Ä‘á»™ng": [
        "rung máº¡nh", "rung báº¥t thÆ°á»ng", "rung láº¯c", "rung láº¯c máº¡nh",
        "rung", "rung nháº¹", "rung liÃªn tá»¥c", "dao Ä‘á»™ng máº¡nh",
        "giáº­t", "giáº­t cá»¥c", "lung lay", "xÃ³c",
    ],
    "Ã‚m thanh": [
        "tiáº¿ng kim loáº¡i va cháº¡m", "tiáº¿ng kim loáº¡i", "tiáº¿ng kÃªu láº¡",
        "tiáº¿ng kÃªu báº¥t thÆ°á»ng", "á»“n báº¥t thÆ°á»ng", "á»“n lá»›n", "tiáº¿ng á»“n láº¡",
        "tiáº¿ng rÃ­t", "tiáº¿ng rÃ­t cao", "tiáº¿ng cá» sÃ¡t", "tiáº¿ng va Ä‘áº­p",
        "tiáº¿ng Ã¹", "tiáº¿ng láº¡", "tiáº¿ng káº¹t", "tiáº¿ng ná»•", "ná»•",
        "tiáº¿ng cáº¡ch cáº¡ch", "tiáº¿ng lÃ¡ch cÃ¡ch", "gáº§m",
    ],
    "MÃ¹i": [
        "mÃ¹i khÃ©t", "mÃ¹i chÃ¡y", "khÃ©t", "mÃ¹i dáº§u chÃ¡y", "mÃ¹i dáº§u",
        "mÃ¹i nhá»›t chÃ¡y", "mÃ¹i cao su chÃ¡y", "mÃ¹i nhá»±a chÃ¡y",
        "mÃ¹i láº¡", "mÃ¹i hÃ´i", "mÃ¹i háº¯c", "bá»‘c mÃ¹i", "chÃ¡y khÃ©t",
    ],
    "Äiá»‡n": [
        "dÃ²ng Ä‘iá»‡n tÄƒng Ä‘á»™t ngá»™t", "dÃ²ng Ä‘iá»‡n tÄƒng", "dÃ²ng Ä‘iá»‡n dao Ä‘á»™ng",
        "dÃ²ng Ä‘iá»‡n báº¥t thÆ°á»ng", "cháº­p Ä‘iá»‡n", "Ä‘Ã¡nh lá»­a", "phÃ³ng Ä‘iá»‡n",
        "tia lá»­a", "tia lá»­a Ä‘iá»‡n", "Ä‘iá»‡n giáº­t", "rÃ² Ä‘iá»‡n", "cháº­p máº¡ch",
        "chÃ¡y cáº§u chÃ¬", "quÃ¡ táº£i", "sá»¥t Ã¡p", "máº¥t pha", "lá»‡ch pha",
    ],
    "RÃ² rá»‰": [
        "rÃ² rá»‰ dáº§u", "rÃ² rá»‰ nÆ°á»›c", "rÃ² rá»‰", "cháº£y dáº§u", "cháº£y nÆ°á»›c",
        "rá»‰ dáº§u", "dáº§u loang", "xÃ¬", "xÃ¬ hÆ¡i", "trÃ n dáº§u",
    ],
    "CÆ¡ khÃ­": [
        "gÃ£y", "ná»©t", "vá»¡", "mÃ²n", "mÃ²n nhiá»u", "Äƒn mÃ²n", "gá»‰ sÃ©t",
        "han gá»‰", "biáº¿n dáº¡ng", "cong vÃªnh", "lá»ng", "lá»ng bu lÃ´ng",
        "tuá»™t", "Ä‘á»©t dÃ¢y Ä‘ai", "dÃ¢y Ä‘ai mÃ²n", "báº¡c Ä‘áº¡n há»ng",
        "báº¡c Ä‘áº¡n", "vÃ²ng bi", "vÃ²ng bi há»ng", "trá»¥c bá»‹ cong", "trá»¥c lá»‡ch",
    ],
    "Hiá»‡u suáº¥t": [
        "cháº¡y cháº­m", "yáº¿u", "cÃ´ng suáº¥t giáº£m", "khÃ´ng khá»Ÿi Ä‘á»™ng",
        "khÃ´ng cháº¡y", "dá»«ng Ä‘á»™t ngá»™t", "táº¯t Ä‘á»™t ngá»™t", "cháº­p chá»n",
        "khÃ´ng á»•n Ä‘á»‹nh", "hoáº¡t Ä‘á»™ng cháº­m", "káº¹t", "treo", "quÃ¡ tá»‘c",
    ],
}

# Vietnamese negation words
NEGATION_WORDS = [
    "khÃ´ng cÃ³", "khÃ´ng bá»‹", "khÃ´ng tháº¥y", "khÃ´ng nghe",
    "khÃ´ng phÃ¡t hiá»‡n", "khÃ´ng cÃ²n", "khÃ´ng há»",
    "chÆ°a cÃ³", "chÆ°a bá»‹", "chÆ°a tháº¥y", "chÆ°a phÃ¡t hiá»‡n",
    "háº¿t", "Ä‘Ã£ háº¿t", "khÃ´ng", "chÆ°a",
]


# ============================================================
# 4. RECOMMENDATION DATABASE
# ============================================================

RECOMMENDATIONS_DB = {
    "ChÃ¡y cuá»™n dÃ¢y / chÃ¡y motor": [
        "ğŸš¨ Dá»ªNG THIáº¾T Bá»Š NGAY Láº¬P Tá»¨C",
        "Ngáº¯t nguá»“n Ä‘iá»‡n vÃ  Ä‘áº£m báº£o an toÃ n khu vá»±c",
        "Kiá»ƒm tra cÃ¡ch Ä‘iá»‡n cuá»™n dÃ¢y (megger test)",
        "Kiá»ƒm tra há»‡ thá»‘ng lÃ m mÃ¡t vÃ  quáº¡t giÃ³",
        "ÄÃ¡nh giÃ¡ láº¡i Ä‘iá»u kiá»‡n táº£i â€” cÃ³ thá»ƒ quÃ¡ táº£i",
        "LiÃªn há»‡ ká»¹ sÆ° Ä‘iá»‡n Ä‘á»ƒ kiá»ƒm tra chuyÃªn sÃ¢u",
    ],
    "Há»ng báº¡c Ä‘áº¡n / vÃ²ng bi": [
        "ğŸš¨ Dá»ªNG THIáº¾T Bá»Š Ä‘á»ƒ trÃ¡nh hÆ° há»ng thÃªm",
        "Kiá»ƒm tra báº¡c Ä‘áº¡n / vÃ²ng bi â€” thay tháº¿ náº¿u cáº§n",
        "Kiá»ƒm tra há»‡ thá»‘ng bÃ´i trÆ¡n â€” bá»• sung má»¡ bÃ´i trÆ¡n",
        "Kiá»ƒm tra Ä‘á»™ Ä‘á»“ng trá»¥c (alignment) cÃ¡c khá»›p ná»‘i",
        "Kiá»ƒm tra cÃ¢n báº±ng Ä‘á»™ng rotor",
    ],
    "QuÃ¡ táº£i cÆ¡ khÃ­": [
        "ğŸš¨ GIáº¢M Táº¢I NGAY hoáº·c dá»«ng thiáº¿t bá»‹",
        "Kiá»ƒm tra Ä‘iá»u kiá»‡n táº£i hiá»‡n táº¡i so vá»›i thÃ´ng sá»‘ thiáº¿t káº¿",
        "Kiá»ƒm tra há»‡ thá»‘ng truyá»n Ä‘á»™ng (dÃ¢y Ä‘ai, khá»›p ná»‘i, há»™p sá»‘)",
        "Kiá»ƒm tra há»‡ thá»‘ng lÃ m mÃ¡t",
        "ÄÃ¡nh giÃ¡ láº¡i quy trÃ¬nh váº­n hÃ nh",
    ],
    "Sá»± cá»‘ Ä‘iá»‡n": [
        "ğŸš¨ NGáº®T NGUá»’N ÄIá»†N NGAY",
        "Kiá»ƒm tra cÃ¡ch Ä‘iá»‡n toÃ n bá»™ há»‡ thá»‘ng",
        "Kiá»ƒm tra Ä‘iá»‡n Ã¡p, dÃ²ng Ä‘iá»‡n, há»‡ sá»‘ cÃ´ng suáº¥t",
        "Kiá»ƒm tra tá»§ Ä‘iá»‡n, CB, contactor, relay báº£o vá»‡",
        "Kiá»ƒm tra tiáº¿p Ä‘á»‹a vÃ  há»‡ thá»‘ng báº£o vá»‡",
        "LiÃªn há»‡ ká»¹ sÆ° Ä‘iá»‡n chuyÃªn trÃ¡ch",
    ],
    "QuÃ¡ nhiá»‡t": [
        "Giáº£m táº£i hoáº·c dá»«ng thiáº¿t bá»‹ Ä‘á»ƒ háº¡ nhiá»‡t",
        "Kiá»ƒm tra há»‡ thá»‘ng lÃ m mÃ¡t (quáº¡t, nÆ°á»›c, dáº§u)",
        "Kiá»ƒm tra Ä‘iá»u kiá»‡n mÃ´i trÆ°á»ng (thÃ´ng giÃ³, nhiá»‡t Ä‘á»™ xung quanh)",
        "Kiá»ƒm tra há»‡ thá»‘ng bÃ´i trÆ¡n",
        "Theo dÃµi nhiá»‡t Ä‘á»™ báº±ng camera nhiá»‡t náº¿u cÃ³",
    ],
    "RÃ² rá»‰ há»‡ thá»‘ng": [
        "XÃ¡c Ä‘á»‹nh vá»‹ trÃ­ rÃ² rá»‰ chÃ­nh xÃ¡c",
        "Kiá»ƒm tra gioÄƒng, phá»›t, seal â€” thay tháº¿ náº¿u há»ng",
        "Kiá»ƒm tra Ã¡p suáº¥t há»‡ thá»‘ng",
        "Bá»• sung dáº§u/nÆ°á»›c náº¿u thiáº¿u",
        "LÃªn káº¿ hoáº¡ch báº£o trÃ¬ thay tháº¿ seal",
    ],
    "HÆ° há»ng cÆ¡ khÃ­": [
        "Kiá»ƒm tra chi tiáº¿t bá»™ pháº­n bá»‹ hÆ° há»ng",
        "ÄÃ¡nh giÃ¡ má»©c Ä‘á»™ hÆ° há»ng â€” sá»­a chá»¯a hoáº·c thay tháº¿",
        "Kiá»ƒm tra cÃ¡c bá»™ pháº­n liÃªn quan cÃ³ bá»‹ áº£nh hÆ°á»Ÿng khÃ´ng",
        "LÃªn káº¿ hoáº¡ch thay tháº¿ phá»¥ tÃ¹ng",
        "RÃ  soÃ¡t láº¡i quy trÃ¬nh báº£o trÃ¬ Ä‘á»‹nh ká»³",
    ],
    "Ã‚m thanh báº¥t thÆ°á»ng": [
        "XÃ¡c Ä‘á»‹nh vá»‹ trÃ­ vÃ  Ä‘áº·c Ä‘iá»ƒm Ã¢m thanh",
        "Kiá»ƒm tra cÃ¡c bá»™ pháº­n quay: báº¡c Ä‘áº¡n, trá»¥c, bÃ¡nh rÄƒng",
        "Kiá»ƒm tra há»‡ thá»‘ng truyá»n Ä‘á»™ng (dÃ¢y Ä‘ai, xÃ­ch)",
        "Kiá»ƒm tra lá»ng káº¿t ná»‘i cÆ¡ khÃ­",
        "Sá»­ dá»¥ng stethoscope cÃ´ng nghiá»‡p Ä‘á»ƒ cháº©n Ä‘oÃ¡n chÃ­nh xÃ¡c",
    ],
    "Giáº£m hiá»‡u suáº¥t": [
        "Kiá»ƒm tra Ä‘iá»u kiá»‡n Ä‘áº§u vÃ o (Ä‘iá»‡n, khÃ­, nÆ°á»›c)",
        "Kiá»ƒm tra bá»™ lá»c â€” vá»‡ sinh hoáº·c thay tháº¿",
        "Kiá»ƒm tra há»‡ thá»‘ng truyá»n Ä‘á»™ng â€” dÃ¢y Ä‘ai, ly há»£p",
        "ÄÃ¡nh giÃ¡ láº¡i thÃ´ng sá»‘ váº­n hÃ nh",
        "LÃªn káº¿ hoáº¡ch báº£o trÃ¬ tá»•ng thá»ƒ",
    ],
    "_default": [
        "Tiáº¿p tá»¥c theo dÃµi thiáº¿t bá»‹",
        "Báº£o dÆ°á»¡ng Ä‘á»‹nh ká»³ theo káº¿ hoáº¡ch",
        "Ghi nháº­n tÃ¬nh tráº¡ng Ä‘á»ƒ theo dÃµi xu hÆ°á»›ng",
    ],
}


# ============================================================
# 5. DATA CLASSES
# ============================================================

@dataclass
class AnalysisResult:
    """Káº¿t quáº£ phÃ¢n tÃ­ch NLP."""
    fault_type: str
    severity: str
    severity_score: float
    confidence: float
    keywords: list = field(default_factory=list)
    symptoms: list = field(default_factory=list)
    recommendations: list = field(default_factory=list)
    summary: str = ""
    pipeline_steps: list = field(default_factory=list)


# ============================================================
# 6. NLP ENGINE CLASS (PhoBERT-based)
# ============================================================

class NLPEngine:
    """
    NLP Engine sá»­ dá»¥ng PhoBERT cho phÃ¢n tÃ­ch thiáº¿t bá»‹ cÃ´ng nghiá»‡p.

    Pipeline:
    1. Tiá»n xá»­ lÃ½ vÄƒn báº£n (normalize, clean)
    2. PhoBERT Tokenization & Encoding
    3. Cosine Similarity vá»›i cÃ¡c máº«u lá»—i tham chiáº¿u
    4. PhÃ¢n loáº¡i lá»—i (semantic classification)
    5. TrÃ­ch xuáº¥t keyword (supplementary)
    6. ÄÃ¡nh giÃ¡ severity
    7. Sinh khuyáº¿n nghá»‹
    """

    def __init__(self):
        self.tokenizer = _tokenizer
        self.model = _model
        self.device = _device
        self.fault_refs = FAULT_REFERENCES
        self.recommendations_db = RECOMMENDATIONS_DB

        # Pre-compute embeddings cho cÃ¡c máº«u tham chiáº¿u
        self.ref_embeddings = {}
        self._precompute_reference_embeddings()

    def _precompute_reference_embeddings(self):
        """TÃ­nh trÆ°á»›c embeddings cho táº¥t cáº£ máº«u tham chiáº¿u."""
        print("ğŸ”„ Pre-computing reference embeddings...")
        for fault_name, fault_data in self.fault_refs.items():
            embeddings = []
            for sample in fault_data["samples"]:
                emb = self._encode_text(sample)
                embeddings.append(emb)
            # Láº¥y trung bÃ¬nh cÃ¡c embeddings lÃ m Ä‘áº¡i diá»‡n cho loáº¡i lá»—i
            self.ref_embeddings[fault_name] = torch.stack(embeddings).mean(dim=0)
        print("âœ… Reference embeddings ready")

    # ----------------------------------------------------------
    # PhoBERT Encoding
    # ----------------------------------------------------------
    @torch.no_grad()
    def _encode_text(self, text: str) -> torch.Tensor:
        """
        Encode text thÃ nh embedding vector sá»­ dá»¥ng PhoBERT.
        Tráº£ vá» [CLS] token embedding (768-dim).
        """
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=256,
        ).to(self.device)

        outputs = self.model(**inputs)
        # Láº¥y [CLS] token (vá»‹ trÃ­ 0) tá»« last hidden state
        cls_embedding = outputs.last_hidden_state[:, 0, :]
        return cls_embedding.squeeze(0).cpu()

    # ----------------------------------------------------------
    # Step 1: Text Preprocessing
    # ----------------------------------------------------------
    def preprocess(self, text: str) -> str:
        """Tiá»n xá»­ lÃ½ vÄƒn báº£n tiáº¿ng Viá»‡t."""
        text = unicodedata.normalize("NFC", text)
        text = text.lower()
        text = re.sub(r'[^\w\sÃ Ã¡áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã¨Ã©áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»©á»«á»­á»¯á»±á»³Ã½á»·á»¹á»µÄ‘]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    # ----------------------------------------------------------
    # Step 2: Keyword Extraction (supplementary)
    # ----------------------------------------------------------
    def extract_keywords(self, text: str) -> list:
        """
        TrÃ­ch xuáº¥t tá»« khÃ³a triá»‡u chá»©ng tá»« text.
        Bá»• sung cho PhoBERT â€” giÃºp hiá»ƒn thá»‹ keywords cho user.
        """
        found = []
        negated_spans = []

        # Collect all keywords, sort by length desc
        all_kw = []
        for category, keywords in SYMPTOM_KEYWORDS.items():
            for kw in keywords:
                all_kw.append((kw, category))
        all_kw.sort(key=lambda x: len(x[0]), reverse=True)

        matched_spans = []

        for kw, category in all_kw:
            pattern = re.compile(re.escape(kw), re.IGNORECASE)
            for match in pattern.finditer(text):
                start, end = match.span()

                # Check if in negated span
                in_negated = any(start >= ns and end <= ne for ns, ne in negated_spans)
                if in_negated:
                    continue

                # Check negation
                lookback = max(0, start - 25)
                preceding = text[lookback:start].strip().lower()
                is_neg = any(preceding.endswith(neg) for neg in NEGATION_WORDS)
                if is_neg:
                    negated_spans.append((start, end))
                    continue

                # Check overlap
                is_overlap = any(start < me and end > ms for ms, me in matched_spans)
                if not is_overlap:
                    matched_spans.append((start, end))
                    found.append({"keyword": kw, "category": category})

        return found

    # ----------------------------------------------------------
    # Step 3: Semantic Fault Classification (PhoBERT)
    # ----------------------------------------------------------
    def classify_fault_phobert(self, text: str) -> list:
        """
        PhÃ¢n loáº¡i lá»—i báº±ng PhoBERT cosine similarity.
        Returns: danh sÃ¡ch (fault_name, similarity_score) Ä‘Ã£ sáº¯p xáº¿p giáº£m dáº§n.
        """
        text_embedding = self._encode_text(text)

        scores = []
        for fault_name, ref_emb in self.ref_embeddings.items():
            similarity = torch.nn.functional.cosine_similarity(
                text_embedding.unsqueeze(0),
                ref_emb.unsqueeze(0),
            ).item()
            scores.append((fault_name, similarity))

        # Sáº¯p xáº¿p giáº£m dáº§n theo similarity
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores

    # ----------------------------------------------------------
    # Step 4: Severity Assessment
    # ----------------------------------------------------------
    def assess_severity(self, fault_type: str, similarity: float, keywords: list) -> tuple:
        """
        ÄÃ¡nh giÃ¡ má»©c Ä‘á»™ nghiÃªm trá»ng.
        Káº¿t há»£p: PhoBERT similarity + severity_base cá»§a loáº¡i lá»—i + sá»‘ keyword.
        """
        fault_data = self.fault_refs.get(fault_type, {})
        severity_base = fault_data.get("severity_base", 0.3)

        # Káº¿t há»£p severity: base * similarity + keyword bonus
        keyword_bonus = min(len(keywords) * 0.05, 0.2)
        severity_score = min(severity_base * similarity + keyword_bonus, 1.0)
        severity_score = round(severity_score, 2)

        if severity_score >= 0.65:
            return ("NGHIÃŠM TRá»ŒNG", severity_score)
        elif severity_score >= 0.40:
            return ("Cáº¢NH BÃO", severity_score)
        else:
            return ("THáº¤P", severity_score)

    # ----------------------------------------------------------
    # Step 5: Summary Generation
    # ----------------------------------------------------------
    def generate_summary(self, equipment: str, fault_type: str, severity: str, keywords: list, similarity: float) -> str:
        """Táº¡o tÃ³m táº¯t."""
        if fault_type == "Hoáº¡t Ä‘á»™ng á»•n Ä‘á»‹nh":
            return f"{equipment} â€” khÃ´ng phÃ¡t hiá»‡n triá»‡u chá»©ng báº¥t thÆ°á»ng. Thiáº¿t bá»‹ cÃ³ thá»ƒ Ä‘ang hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng."

        fault_desc = self.fault_refs.get(fault_type, {}).get("description", fault_type)
        kw_list = ", ".join([k["keyword"] for k in keywords]) if keywords else "khÃ´ng rÃµ"

        summary = f"{equipment} â€” PhoBERT phÃ¢n loáº¡i: {fault_type} (similarity: {similarity:.1%}). "
        summary += f"Tá»« khÃ³a phÃ¡t hiá»‡n: {kw_list}. "
        summary += f"MÃ´ táº£: {fault_desc}. Má»©c Ä‘á»™: {severity}."
        return summary

    # ----------------------------------------------------------
    # MAIN PIPELINE
    # ----------------------------------------------------------
    def analyze(self, equipment: str, description: str) -> AnalysisResult:
        """
        Main NLP pipeline sá»­ dá»¥ng PhoBERT.

        Args:
            equipment: Loáº¡i thiáº¿t bá»‹
            description: MÃ´ táº£ tá»± nhiÃªn tiáº¿ng Viá»‡t

        Returns:
            AnalysisResult
        """
        pipeline_steps = []

        # Step 1: Preprocessing
        cleaned = self.preprocess(description)
        pipeline_steps.append({
            "step": 1,
            "name": "Tiá»n xá»­ lÃ½ vÄƒn báº£n",
            "input": description,
            "output": cleaned,
        })

        # Step 2: PhoBERT Tokenization
        tokens = self.tokenizer.tokenize(cleaned)
        pipeline_steps.append({
            "step": 2,
            "name": "PhoBERT Tokenization",
            "input": cleaned,
            "output": tokens,
        })

        # Step 3: Keyword Extraction
        keywords = self.extract_keywords(cleaned)
        pipeline_steps.append({
            "step": 3,
            "name": "TrÃ­ch xuáº¥t tá»« khÃ³a",
            "input": cleaned,
            "output": keywords,
        })

        # Step 4: PhoBERT Semantic Classification
        scores = self.classify_fault_phobert(cleaned)
        top_5 = scores[:5]

        pipeline_steps.append({
            "step": 4,
            "name": "PhoBERT PhÃ¢n loáº¡i lá»—i (Cosine Similarity)",
            "input": "PhoBERT embedding (768-dim)",
            "output": [{
                "fault": ("âœ… " if self.fault_refs.get(f, {}).get("is_normal", False) else "âš ï¸ ") + f,
                "similarity": round(s, 4),
            } for f, s in top_5],
        })

        # --- KEYWORD-AWARE RE-RANKING ---
        # Káº¿t há»£p PhoBERT similarity vá»›i keyword detection
        # Náº¿u cÃ³ keywords thuá»™c category nÃ o â†’ boost fault type liÃªn quan
        keyword_categories = set(k["category"] for k in keywords)

        # Mapping category â†’ related fault types
        CATEGORY_FAULT_MAP = {
            "Nhiá»‡t Ä‘á»™": ["QuÃ¡ nhiá»‡t", "ChÃ¡y cuá»™n dÃ¢y / chÃ¡y motor", "QuÃ¡ táº£i cÆ¡ khÃ­"],
            "Rung Ä‘á»™ng": ["Há»ng báº¡c Ä‘áº¡n / vÃ²ng bi", "QuÃ¡ táº£i cÆ¡ khÃ­"],
            "Ã‚m thanh": ["Há»ng báº¡c Ä‘áº¡n / vÃ²ng bi", "Ã‚m thanh báº¥t thÆ°á»ng"],
            "MÃ¹i": ["ChÃ¡y cuá»™n dÃ¢y / chÃ¡y motor"],
            "Äiá»‡n": ["Sá»± cá»‘ Ä‘iá»‡n", "ChÃ¡y cuá»™n dÃ¢y / chÃ¡y motor"],
            "RÃ² rá»‰": ["RÃ² rá»‰ há»‡ thá»‘ng"],
            "CÆ¡ khÃ­": ["HÆ° há»ng cÆ¡ khÃ­", "Há»ng báº¡c Ä‘áº¡n / vÃ²ng bi"],
            "Hiá»‡u suáº¥t": ["Giáº£m hiá»‡u suáº¥t", "QuÃ¡ táº£i cÆ¡ khÃ­"],
        }

        # Boost scores dá»±a trÃªn keyword categories
        boosted_scores = []
        for fault_name, sim in scores:
            boost = 0.0
            for cat in keyword_categories:
                related = CATEGORY_FAULT_MAP.get(cat, [])
                if fault_name in related:
                    boost += 0.1  # Boost 0.1 cho má»—i category match
            boosted_scores.append((fault_name, sim + boost))

        boosted_scores.sort(key=lambda x: x[1], reverse=True)
        top_fault, top_score = boosted_scores[0]
        top_sim = dict(scores)[top_fault]  # Original similarity (khÃ´ng boost)

        # --- DECISION LOGIC ---
        # Kiá»ƒm tra náº¿u káº¿t quáº£ lÃ  "BÃ¬nh thÆ°á»ng" HOáº¶C khÃ´ng cÃ³ keyword nÃ o
        is_normal = self.fault_refs.get(top_fault, {}).get("is_normal", False)

        if is_normal or (len(keywords) == 0):
            fault_type = "Hoáº¡t Ä‘á»™ng á»•n Ä‘á»‹nh"
            severity = "THáº¤P"
            severity_score = 0.0
            confidence = round(top_sim, 2)
            recommendations = RECOMMENDATIONS_DB["_default"]
        else:
            fault_type = top_fault
            severity, severity_score = self.assess_severity(fault_type, top_sim, keywords)
            confidence = round(top_sim, 2)
            recommendations = self.recommendations_db.get(fault_type, self.recommendations_db["_default"])

        pipeline_steps.append({
            "step": 5,
            "name": "ÄÃ¡nh giÃ¡ má»©c Ä‘á»™ nghiÃªm trá»ng",
            "input": f"fault={fault_type}, similarity={top_sim:.4f}, keywords={len(keywords)}",
            "output": {"severity": severity, "score": severity_score},
        })

        # Step 6: Recommendations
        pipeline_steps.append({
            "step": 6,
            "name": "Sinh khuyáº¿n nghá»‹",
            "input": fault_type,
            "output": recommendations,
        })

        # Summary
        summary = self.generate_summary(equipment, fault_type, severity, keywords, top_sim)

        return AnalysisResult(
            fault_type=fault_type,
            severity=severity,
            severity_score=severity_score,
            confidence=confidence,
            keywords=[k["keyword"] for k in keywords],
            symptoms=[{
                "keyword": k["keyword"],
                "category": k["category"],
                "label": k["keyword"],
                "weight": 3,
            } for k in keywords],
            recommendations=recommendations,
            summary=summary,
            pipeline_steps=pipeline_steps,
        )


# ============================================================
# SINGLETON & CONVENIENCE
# ============================================================

engine = NLPEngine()


def analyze(equipment: str, description: str) -> AnalysisResult:
    """Convenience function â€” gá»i engine.analyze()."""
    return engine.analyze(equipment, description)
