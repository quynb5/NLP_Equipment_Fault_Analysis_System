# ğŸ”¬ Fine-tuning PhoBERT cho Text Classification

## 1. Ká»¹ thuáº­t sá»­ dá»¥ng

Ká»¹ thuáº­t nÃ y gá»i lÃ  **Transfer Learning** (Há»c chuyá»ƒn giao), cá»¥ thá»ƒ lÃ  **Fine-tuning a Pre-trained Language Model** cho bÃ i toÃ¡n **downstream task** (Text Classification).

> **"Fine-tuning BERT-based models for sequence classification"**
> (Devlin et al., 2019 â€” paper gá»‘c BERT)

### Transfer Learning lÃ  gÃ¬?

Transfer Learning lÃ  phÆ°Æ¡ng phÃ¡p **táº­n dá»¥ng kiáº¿n thá»©c** Ä‘Ã£ há»c tá»« má»™t task/domain lá»›n (pre-training) Ä‘á»ƒ Ã¡p dá»¥ng cho má»™t task/domain nhá» hÆ¡n, cá»¥ thá»ƒ hÆ¡n (downstream task). Thay vÃ¬ train model tá»« Ä‘áº§u (from scratch) vá»›i random weights, ta báº¯t Ä‘áº§u tá»« model Ä‘Ã£ cÃ³ sáºµn kiáº¿n thá»©c ngÃ´n ngá»¯ â†’ chá»‰ cáº§n **tinh chá»‰nh (fine-tune)** cho bÃ i toÃ¡n cá»¥ thá»ƒ.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Transfer Learning Pipeline                    â”‚
â”‚                                                         â”‚
â”‚  Phase 1: Pre-training (Ä‘Ã£ lÃ m sáºµn bá»Ÿi VinAI)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Dá»¯ liá»‡u: 20GB text tiáº¿ng Viá»‡t              â”‚        â”‚
â”‚  â”‚ Task: Masked Language Modeling (MLM)         â”‚        â”‚
â”‚  â”‚ Output: PhoBERT-base (135M parameters)       â”‚        â”‚
â”‚  â”‚ â†’ Kiáº¿n thá»©c ngÃ´n ngá»¯ tiáº¿ng Viá»‡t phong phÃº   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                      â†“                                  â”‚
â”‚  Phase 2: Fine-tuning (do chÃºng ta thá»±c hiá»‡n)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Dá»¯ liá»‡u: 1,197 máº«u lá»—i thiáº¿t bá»‹ cÃ³ nhÃ£n   â”‚        â”‚
â”‚  â”‚ Task: Fault Classification (10 classes)      â”‚        â”‚
â”‚  â”‚ ThÃªm: Linear classification head             â”‚        â”‚
â”‚  â”‚ â†’ Model chuyÃªn biá»‡t phÃ¢n loáº¡i lá»—i thiáº¿t bá»‹  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Quy trÃ¬nh Fine-tuning Ä‘Ã£ thá»±c hiá»‡n

### BÆ°á»›c 1 â€” Sá»­ dá»¥ng Pre-trained PhoBERT

PhoBERT (`vinai/phobert-base`) lÃ  model Transformer Ä‘Ã£ Ä‘Æ°á»£c **pre-train** trÃªn **20GB dá»¯ liá»‡u tiáº¿ng Viá»‡t** (bÃ¡o, wiki, forum...) báº±ng bÃ i toÃ¡n **Masked Language Modeling (MLM)** â€” che má»™t sá»‘ tá»« trong cÃ¢u rá»“i train model Ä‘oÃ¡n láº¡i:

```
Input:  "Motor bá»‹ [MASK] báº¥t thÆ°á»ng khi váº­n hÃ nh"
Output: "Motor bá»‹  rung  báº¥t thÆ°á»ng khi váº­n hÃ nh"
```

Sau pre-training, PhoBERT Ä‘Ã£ cÃ³ **kiáº¿n thá»©c ngÃ´n ngá»¯ tiáº¿ng Viá»‡t**:
- Hiá»ƒu ngá»¯ phÃ¡p, cÃº phÃ¡p tiáº¿ng Viá»‡t
- Hiá»ƒu quan há»‡ ngá»¯ nghÄ©a giá»¯a cÃ¡c tá»«
- Encode báº¥t ká»³ cÃ¢u tiáº¿ng Viá»‡t nÃ o thÃ nh **vector 768 chiá»u** chá»©a thÃ´ng tin ngá»¯ nghÄ©a

### BÆ°á»›c 2 â€” ThÃªm Classification Head

ThÃªm **1 layer Linear** lÃªn trÃªn PhoBERT Ä‘á»ƒ biáº¿n nÃ³ thÃ nh classifier:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PhoBERTClassifier (nn.Module)                    â”‚
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ PhoBERT Encoder (vinai/phobert-base)       â”‚  â”‚
â”‚  â”‚   12 Transformer layers                    â”‚  â”‚
â”‚  â”‚   Hidden size: 768                         â”‚  â”‚
â”‚  â”‚   Attention heads: 12                      â”‚  â”‚
â”‚  â”‚   Parameters: ~135M (pre-trained)          â”‚  â”‚
â”‚  â”‚   â„ï¸ Embedding layer: FROZEN              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚ [CLS] token (768-dim)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Dropout(p=0.3) â€” Regularization            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Linear(768, 10) â€” Classification Head      â”‚  â”‚
â”‚  â”‚   Parameters: 768 Ã— 10 + 10 = 7,690       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚ logits (10-dim)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Softmax â†’ Probabilities (10 classes)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### BÆ°á»›c 3 â€” Train trÃªn dá»¯ liá»‡u cÃ³ nhÃ£n

| ThÃ nh pháº§n | Chi tiáº¿t |
|---|---|
| **Dá»¯ liá»‡u** | 1,197 máº«u (957 train / 240 val) |
| **Optimizer** | AdamW (Adam with decoupled weight decay) |
| **Learning Rate** | 2e-5 (standard cho fine-tuning Transformer) |
| **Scheduler** | Linear warmup (10% steps) + Cosine decay |
| **Early Stopping** | Patience=5 (dá»«ng náº¿u 5 epochs khÃ´ng cáº£i thiá»‡n) |
| **Freeze** | Embedding layer frozen (tiáº¿t kiá»‡m memory) |
| **Best Epoch** | 4/20 (val accuracy = 100%) |
| **Training Time** | ~150 giÃ¢y (GPU) |

### BÆ°á»›c 4 â€” Inference (Sá»­ dá»¥ng)

```
Input: "Motor nÃ³ng báº¥t thÆ°á»ng, rung máº¡nh"
  â†’ PhoBERT Tokenize â†’ [CLS] motor nÃ³ng báº¥t_thÆ°á»ng , rung máº¡nh [SEP]
  â†’ PhoBERT Encode   â†’ [CLS] embedding (768-dim)
  â†’ Dropout           â†’ Regularization
  â†’ Linear(768, 10)  â†’ 10 logits
  â†’ Softmax           â†’ [0.01, 0.01, 0.01, 0.01, 0.01, 0.92, 0.01, 0.01, 0.01, 0.01]
  â†’ Output: "QuÃ¡ nhiá»‡t" (confidence=0.92)
```

---

## 3. Æ¯u Ä‘iá»ƒm cá»§a Transfer Learning / Fine-tuning

### 3.1 Tiáº¿t kiá»‡m dá»¯ liá»‡u (Data Efficiency)

| PhÆ°Æ¡ng phÃ¡p | Dá»¯ liá»‡u cáº§n |
|---|:---:|
| Train from scratch | 10,000 - 100,000+ máº«u |
| **Fine-tuning PhoBERT** | **~1,200 máº«u** |

Pre-training trÃªn 20GB text â†’ model Ä‘Ã£ cÃ³ kiáº¿n thá»©c ngÃ´n ngá»¯. Chá»‰ cáº§n Ã­t dá»¯ liá»‡u labeled Ä‘á»ƒ "dáº¡y" thÃªm bÃ i toÃ¡n cá»¥ thá»ƒ. ÄÃ¢y lÃ  Æ°u Ä‘iá»ƒm **quan trá»ng nháº¥t** trong thá»±c táº¿ vÃ¬ thu tháº­p dá»¯ liá»‡u labeled ráº¥t tá»‘n kÃ©m.

### 3.2 Hiá»ƒu ngá»¯ nghÄ©a sÃ¢u (Semantic Understanding)

```
TF-IDF:    "váº¿t áº©m Æ°á»›t lan rá»™ng"  â†’  khÃ´ng match keyword "rÃ² rá»‰"  â†’  âŒ Sai
PhoBERT:   "váº¿t áº©m Æ°á»›t lan rá»™ng"  â†’  embedding gáº§n "rÃ² rá»‰"       â†’  âœ… ÄÃºng
```

- PhoBERT hiá»ƒu **paraphrase**: "váº¿t áº©m Æ°á»›t" â‰ˆ "rÃ² rá»‰"
- Hiá»ƒu **world knowledge**: "bi vá»¡ máº»" â†’ liÃªn quan Ä‘áº¿n báº¡c Ä‘áº¡n
- TF-IDF chá»‰ match **exact keyword**, khÃ´ng hiá»ƒu ngá»¯ nghÄ©a

### 3.3 Contextual Representation (Biá»ƒu diá»…n phá»¥ thuá»™c ngá»¯ cáº£nh)

Má»—i tá»« cÃ³ embedding **khÃ¡c nhau** tÃ¹y vÃ o context xung quanh (nhá» Transformer self-attention):

```
"Motor nÃ³ng báº¥t thÆ°á»ng"    â†’ embedding("nÃ³ng") = vector hÆ°á»›ng fault
"Motor khÃ´ng nÃ³ng"         â†’ embedding("nÃ³ng") = vector hÆ°á»›ng normal (bá»‹ negation)
"KhÃ´ng nÃ³ng khÃ´ng á»“n"      â†’ embedding tá»•ng thá»ƒ hÆ°á»›ng "hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng"
```

TF-IDF: tá»« "nÃ³ng" **luÃ´n cÃ³ cÃ¹ng 1 weight** báº¥t ká»ƒ context â†’ khÃ´ng phÃ¢n biá»‡t Ä‘Æ°á»£c phá»§ Ä‘á»‹nh.

### 3.4 PhÃ¢n biá»‡t Root Cause vs Symptom

```
Input: "á»c siáº¿t bÃ­ch ná»‘i bá»‹ gÃ£y do rung, bÃ­ch há»Ÿ gÃ¢y rÃ² rá»‰ nháº¹"
         ^^^^^^^^^^^^^^^^^^^^^^                   ^^^^^^^^
         Root cause (cÆ¡ khÃ­)                      Symptom (rÃ² rá»‰)

PhoBERT attention nhÃ¬n toÃ n cÃ¢u â†’ hiá»ƒu root cause = cÆ¡ khÃ­  â†’ âœ…
TF-IDF báº¯t keyword "rÃ² rá»‰" (weight cao)                     â†’ âŒ
```

### 3.5 Generalization tá»‘t

Fine-tuned model hoáº¡t Ä‘á»™ng tá»‘t vá»›i cÃ¢u mÃ´ táº£ **chÆ°a tá»«ng tháº¥y** trong training data. Nhá» pre-trained knowledge:
- "Motor kÃ©o khÃ´ng ná»•i" (chÆ°a tháº¥y) â†’ hiá»ƒu â‰ˆ "quÃ¡ táº£i" (Ä‘Ã£ tháº¥y)
- "Tiáº¿ng láº¡ch cáº¡ch tá»« bÃªn trong" (chÆ°a tháº¥y) â†’ hiá»ƒu â‰ˆ "Ã¢m thanh báº¥t thÆ°á»ng" (Ä‘Ã£ tháº¥y)

### 3.6 Há»™i tá»¥ nhanh (Fast Convergence)

```
Epoch 1: Val Acc = 85.0%     â† ÄÃ£ khÃ¡ tá»‘t ngay epoch Ä‘áº§u
Epoch 2: Val Acc = 94.6%
Epoch 3: Val Acc = 98.3%
Epoch 4: Val Acc = 100.0%    â† Best (chá»‰ 4 epochs!)
```

Nhá» pre-trained weights lÃ  **starting point tá»‘t**, khÃ´ng cáº§n train tá»« random initialization (thÆ°á»ng cáº§n 50-100+ epochs).

---

## 4. So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c

| PhÆ°Æ¡ng phÃ¡p | Data cáº§n | Semantic | Accuracy | Tá»‘c Ä‘á»™ | KÃ­ch thÆ°á»›c |
|---|:---:|:---:|:---:|:---:|:---:|
| **Rule-based** (keyword) | 0 | âŒ | ~60% | Ráº¥t nhanh | ~0 |
| **TF-IDF + LR** | ~1,000+ | âŒ | 89.26% | Nhanh (~3ms) | ~50KB |
| **PhoBERT Zero-shot** | 0 | âœ… Má»™t pháº§n | 73.83% | Cháº­m (~30ms) | 540MB |
| **PhoBERT Fine-tuned** âœ… | ~1,200 | âœ… Äáº§y Ä‘á»§ | **89.93%** | Cháº­m (~30ms) | 540MB |
| **Train from scratch** | 10,000+ | âœ… | CÃ³ thá»ƒ cao hÆ¡n | Ráº¥t cháº­m | TÃ¹y |

---

## 5. Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c

| Metric | TrÆ°á»›c Fine-tuning | Sau Fine-tuning | Cáº£i thiá»‡n |
|---|:---:|:---:|:---:|
| **Accuracy** | 73.83% (zero-shot) | **89.93%** | +16.1% |
| **F1 (macro)** | 74.02% | **89.99%** | +16.0% |
| **Misclassified** | 39/149 | **15/149** | -24 máº«u |

So vá»›i TF-IDF baseline:

| Metric | PhoBERT (Fine-tuned) | TF-IDF |
|---|:---:|:---:|
| **Accuracy** | **89.93%** âœ… | 89.26% |
| **F1 (macro)** | **89.99%** | 89.31% |

---

## 6. Thuáº­t ngá»¯ há»c thuáº­t

| Thuáº­t ngá»¯ | Giáº£i thÃ­ch |
|---|---|
| **Transfer Learning** | Há»c chuyá»ƒn giao â€” dÃ¹ng knowledge tá»« task/domain khÃ¡c |
| **Fine-tuning** | Tinh chá»‰nh model pre-trained cho task cá»¥ thá»ƒ |
| **Pre-trained Language Model (PLM)** | Model ngÃ´n ngá»¯ Ä‘Ã£ pre-train (PhoBERT, BERT, GPT...) |
| **Downstream Task** | BÃ i toÃ¡n cá»¥ thá»ƒ cáº§n giáº£i (text classification) |
| **[CLS] Token Pooling** | DÃ¹ng embedding token [CLS] Ä‘áº¡i diá»‡n cho cáº£ cÃ¢u |
| **Masked Language Modeling (MLM)** | BÃ i toÃ¡n pre-training: Ä‘oÃ¡n tá»« bá»‹ che |
| **Transformer / Self-Attention** | Kiáº¿n trÃºc máº¡ng xá»­ lÃ½ song song, tÃ­nh attention giá»¯a má»i cáº·p tá»« |
| **AdamW Optimizer** | Adam with decoupled weight decay â€” optimizer chuáº©n cho Transformers |
| **Learning Rate Warmup** | TÄƒng dáº§n LR á»Ÿ Ä‘áº§u training trÃ¡nh gradient explosion |
| **Cosine Decay** | Giáº£m LR theo hÃ m cosine, mÆ°á»£t hÆ¡n step decay |
| **Early Stopping** | Dá»«ng training sá»›m khi validation metric khÃ´ng cáº£i thiá»‡n |
| **Dropout** | Ká»¹ thuáº­t regularization â€” random táº¯t neurons khi training |

---

## 7. CÃ¡c file liÃªn quan

| File | MÃ´ táº£ |
|---|---|
| `backend/training/train_phobert.py` | Pipeline fine-tuning hoÃ n chá»‰nh |
| `backend/training/config.py` | Hyperparameters |
| `backend/training/data_preparation.py` | Data loading & augmentation |
| `backend/core/phobert_engine.py` | Engine sá»­ dá»¥ng fine-tuned model |
| `backend/resources/phobert-finetuned/` | Model artifacts Ä‘Ã£ train |

---

## 8. Tham kháº£o

1. **PhoBERT** â€” Nguyen & Nguyen (2020): *"PhoBERT: Pre-trained language models for Vietnamese"* â€” VinAI Research
2. **BERT** â€” Devlin et al. (2019): *"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"* â€” Google AI
3. **Transfer Learning Survey** â€” Ruder (2019): *"Neural Transfer Learning for NLP"*
4. **AdamW** â€” Loshchilov & Hutter (2019): *"Decoupled Weight Decay Regularization"*
